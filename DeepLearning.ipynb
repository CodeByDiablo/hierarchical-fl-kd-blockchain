{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2db7J7onZBVM",
        "outputId": "426fd28d-721b-4144-82e6-b983a5925474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV (replace with your actual file path)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/AUV Implenation/combined_fixed_labels.csv\")\n",
        "\n",
        "# Replace 'label' with the actual column name of your labels\n",
        "label_counts = df['label'].value_counts()\n",
        "\n",
        "print(label_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1tDe0yYXMHG",
        "outputId": "b0632a5e-e61f-406f-97a3-ff509ae7153a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "0    19700000\n",
            "1    19700000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc-naGB5bWlI",
        "outputId": "c59c4647-67bf-4d23-9337-d5ea63625586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "def create_balanced_sample(csv_path, output_path, sample_size=1000000, label_col='label', chunksize=100000):\n",
        "    \"\"\"\n",
        "    Create a balanced sample from a large CSV file with equal 0s and 1s in the label column.\n",
        "\n",
        "    Args:\n",
        "        csv_path: Path to the input CSV file\n",
        "        output_path: Path to save the balanced sample\n",
        "        sample_size: Total number of rows in the balanced sample\n",
        "        label_col: Name of the label column\n",
        "        chunksize: Number of rows to read at a time\n",
        "    \"\"\"\n",
        "\n",
        "    # Count total rows and label distribution first\n",
        "    print(\"Counting rows and label distribution...\")\n",
        "    total_rows = 0\n",
        "    label_counts = defaultdict(int)\n",
        "\n",
        "    for chunk in pd.read_csv(csv_path, chunksize=chunksize):\n",
        "        total_rows += len(chunk)\n",
        "        chunk_labels = chunk[label_col].value_counts().to_dict()\n",
        "        for label, count in chunk_labels.items():\n",
        "            # Convert label to int for consistency\n",
        "            if isinstance(label, str) and label.lower() == \"normal\":\n",
        "                label_counts[0] += count\n",
        "            else:\n",
        "                label_counts[int(label)] += count\n",
        "\n",
        "    print(f\"Total rows: {total_rows:,}\")\n",
        "    print(f\"Label distribution: {dict(label_counts)}\")\n",
        "\n",
        "    # Calculate how many of each label we need\n",
        "    samples_per_label = sample_size // 2\n",
        "    print(f\"Need {samples_per_label:,} samples for each label\")\n",
        "\n",
        "    # Check if we have enough samples for each label\n",
        "    for label, count in label_counts.items():\n",
        "        if count < samples_per_label:\n",
        "            print(f\"Warning: Only {count:,} samples available for label {label}, but need {samples_per_label:,}\")\n",
        "            samples_per_label = min(samples_per_label, count)\n",
        "\n",
        "    # Reset to actual sample size\n",
        "    sample_size = samples_per_label * 2\n",
        "    print(f\"Final sample size: {sample_size:,}\")\n",
        "\n",
        "    # Initialize collections for each label\n",
        "    sampled_data = {0: [], 1: []}\n",
        "\n",
        "    # Read through file again and sample rows\n",
        "    print(\"Sampling balanced data...\")\n",
        "    rows_processed = 0\n",
        "\n",
        "    for chunk in pd.read_csv(csv_path, chunksize=chunksize):\n",
        "        # Process each row in the chunk\n",
        "        for idx, row in chunk.iterrows():\n",
        "            # Get label value\n",
        "            label_val = row[label_col]\n",
        "\n",
        "            # Convert label to int (0 for \"normal\", 1 for others)\n",
        "            if isinstance(label_val, str) and label_val.lower() == \"normal\":\n",
        "                label_int = 0\n",
        "            else:\n",
        "                label_int = int(label_val)\n",
        "\n",
        "            # Only consider labels 0 and 1\n",
        "            if label_int not in [0, 1]:\n",
        "                continue\n",
        "\n",
        "            # Check if we need more samples for this label\n",
        "            if len(sampled_data[label_int]) < samples_per_label:\n",
        "                sampled_data[label_int].append(row)\n",
        "\n",
        "            # Check if we have enough samples\n",
        "            if (len(sampled_data[0]) >= samples_per_label and\n",
        "                len(sampled_data[1]) >= samples_per_label):\n",
        "                break\n",
        "\n",
        "        rows_processed += len(chunk)\n",
        "        print(f\"Processed {rows_processed:,} rows. \"\n",
        "              f\"Collected: 0s={len(sampled_data[0]):,}, 1s={len(sampled_data[1]):,}\")\n",
        "\n",
        "        # Break early if we have enough samples\n",
        "        if (len(sampled_data[0]) >= samples_per_label and\n",
        "            len(sampled_data[1]) >= samples_per_label):\n",
        "            break\n",
        "\n",
        "    # Combine the sampled data\n",
        "    print(\"Combining samples...\")\n",
        "    sampled_rows = sampled_data[0] + sampled_data[1]\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    sampled_df = pd.DataFrame(sampled_rows)\n",
        "\n",
        "    # Shuffle the DataFrame\n",
        "    sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Verify the balance\n",
        "    label_dist = sampled_df[label_col].value_counts()\n",
        "    print(f\"Final label distribution:\\n{label_dist}\")\n",
        "\n",
        "    # Save to CSV\n",
        "    print(f\"Saving to {output_path}...\")\n",
        "    sampled_df.to_csv(output_path, index=False)\n",
        "    print(\"Done!\")\n",
        "\n",
        "    return sampled_df\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    input_csv = \"/content/drive/MyDrive/AUV Implenation/combined_fixed_labels.csv\"\n",
        "    output_csv = \"/content/drive/MyDrive/AUV Implenation/balanced_sample_1M.csv\"\n",
        "\n",
        "    # Create the balanced sample\n",
        "    balanced_data = create_balanced_sample(input_csv, output_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OUFJ5-8YVqY2",
        "outputId": "2f1a7475-dcee-4875-c17d-72d9ede897ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting rows and label distribution...\n",
            "Total rows: 39,400,000\n",
            "Label distribution: {1: 19700000, 0: 19700000}\n",
            "Need 500,000 samples for each label\n",
            "Final sample size: 1,000,000\n",
            "Sampling balanced data...\n",
            "Processed 100,000 rows. Collected: 0s=49,911, 1s=50,089\n",
            "Processed 200,000 rows. Collected: 0s=100,000, 1s=100,000\n",
            "Processed 300,000 rows. Collected: 0s=149,937, 1s=150,063\n",
            "Processed 400,000 rows. Collected: 0s=200,000, 1s=200,000\n",
            "Processed 500,000 rows. Collected: 0s=249,963, 1s=250,037\n",
            "Processed 600,000 rows. Collected: 0s=300,000, 1s=300,000\n",
            "Processed 700,000 rows. Collected: 0s=350,113, 1s=349,887\n",
            "Processed 800,000 rows. Collected: 0s=400,000, 1s=400,000\n",
            "Processed 900,000 rows. Collected: 0s=449,978, 1s=450,022\n",
            "Processed 1,000,000 rows. Collected: 0s=500,000, 1s=500,000\n",
            "Combining samples...\n",
            "Final label distribution:\n",
            "label\n",
            "1    500000\n",
            "0    500000\n",
            "Name: count, dtype: int64\n",
            "Saving to /content/drive/MyDrive/AUV Implenation/balanced_sample_1M.csv...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "csv_path = '/content/drive/MyDrive/AUV Implenation/balanced_sample_1M.csv'  # update path\n",
        "label_col = 'label'\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"[INFO] Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\\n\")\n",
        "\n",
        "# ---------------- COLUMN TYPES ----------------\n",
        "print(\"[INFO] Column data types:\")\n",
        "print(df.dtypes)\n",
        "print()\n",
        "\n",
        "# ---------------- MISSING VALUES ----------------\n",
        "print(\"[INFO] Missing values per column:\")\n",
        "print(df.isna().sum())\n",
        "print()\n",
        "\n",
        "# ---------------- LABEL DISTRIBUTION ----------------\n",
        "print(\"[INFO] Label distribution:\")\n",
        "if df[label_col].dtype == object:\n",
        "    print(df[label_col].value_counts())\n",
        "else:\n",
        "    print(df[label_col].value_counts(normalize=True))\n",
        "print()\n",
        "\n",
        "# ---------------- NUMERIC FEATURE STATS ----------------\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if label_col in numeric_cols:\n",
        "    numeric_cols.remove(label_col)\n",
        "\n",
        "print(f\"[INFO] Numeric feature stats ({len(numeric_cols)} columns):\")\n",
        "print(df[numeric_cols].describe())\n",
        "print()\n",
        "\n",
        "# ---------------- CATEGORICAL FEATURE CHECK ----------------\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "if categorical_cols:\n",
        "    print(f\"[INFO] Categorical columns: {categorical_cols}\")\n",
        "    for col in categorical_cols:\n",
        "        print(f\"  {col} unique values: {df[col].nunique()}\")\n",
        "        print(f\"  Sample values: {df[col].unique()[:10]}\")\n",
        "else:\n",
        "    print(\"[INFO] No categorical columns detected.\")\n",
        "\n",
        "# ---------------- OPTIONAL: Check labels are integers 0/1 ----------------\n",
        "if df[label_col].dtype != np.int64 and df[label_col].dtype != np.int32:\n",
        "    print(f\"\\n[WARNING] Label column '{label_col}' is not integer type. Sample values:\")\n",
        "    print(df[label_col].unique()[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teCjrIq3eBDN",
        "outputId": "0faadb62-a1e3-4333-8e22-77f6d472d9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Dataset loaded: 1000000 rows, 7 columns\n",
            "\n",
            "[INFO] Column data types:\n",
            "Date                  object\n",
            "Time (UTC)            object\n",
            "Salinity (ppt)       float64\n",
            "Temperature (°C)     float64\n",
            "Depth (m)            float64\n",
            "Sound speed (m/s)    float64\n",
            "label                  int64\n",
            "dtype: object\n",
            "\n",
            "[INFO] Missing values per column:\n",
            "Date                 0\n",
            "Time (UTC)           0\n",
            "Salinity (ppt)       0\n",
            "Temperature (°C)     0\n",
            "Depth (m)            0\n",
            "Sound speed (m/s)    0\n",
            "label                0\n",
            "dtype: int64\n",
            "\n",
            "[INFO] Label distribution:\n",
            "label\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "[INFO] Numeric feature stats (4 columns):\n",
            "       Salinity (ppt)  Temperature (°C)       Depth (m)  Sound speed (m/s)\n",
            "count  1000000.000000    1000000.000000  1000000.000000     1000000.000000\n",
            "mean        31.085999         11.339168       28.754971        1487.826492\n",
            "std          8.572060          1.228593       20.222632          18.807726\n",
            "min         -2.687974          7.767922       -7.035373        1434.303589\n",
            "25%         30.520000         10.750616       12.200000        1488.805115\n",
            "50%         32.383572         11.354000       28.222628        1492.862168\n",
            "75%         33.614884         12.164635       42.910000        1494.172466\n",
            "max         72.442604         18.000827      143.907977        1578.989630\n",
            "\n",
            "[INFO] Categorical columns: ['Date', 'Time (UTC)']\n",
            "  Date unique values: 2\n",
            "  Sample values: ['15-11-2024' '14-11-2024']\n",
            "  Time (UTC) unique values: 233\n",
            "  Sample values: ['07:07:18' '17:04:53' '17:05:17' '07:06:52' '17:04:33' '17:07:05'\n",
            " '17:06:53' '07:09:36' '17:05:39' '07:07:44']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "input_csv = '/content/drive/MyDrive/AUV Implenation/combined_fixed_labels.csv'\n",
        "output_csv = '/content/drive/MyDrive/AUV Implenation/clean_balanced_10M.csv'\n",
        "n_samples = 10_000_000  # total rows\n",
        "label_col = 'label'\n",
        "\n",
        "# ---------------- LOAD & CLEAN ----------------\n",
        "chunksize = 500_000\n",
        "pos_samples, neg_samples = [], []\n",
        "pos_collected, neg_collected = 0, 0\n",
        "target_per_class = n_samples // 2\n",
        "\n",
        "for chunk in pd.read_csv(input_csv, chunksize=chunksize):\n",
        "    # 1️⃣ Fix numeric negatives\n",
        "    for col in ['Salinity (ppt)', 'Depth (m)']:\n",
        "        chunk[col] = chunk[col].clip(lower=0)\n",
        "\n",
        "    # 2️⃣ Keep only necessary columns\n",
        "    chunk = chunk[['Date', 'Time (UTC)', 'Salinity (ppt)', 'Temperature (°C)', 'Depth (m)', 'Sound speed (m/s)', label_col]]\n",
        "\n",
        "    # 3️⃣ Split by label\n",
        "    pos_chunk = chunk[chunk[label_col] == 1]\n",
        "    neg_chunk = chunk[chunk[label_col] == 0]\n",
        "\n",
        "    if pos_collected < target_per_class and len(pos_chunk) > 0:\n",
        "        take = min(target_per_class - pos_collected, len(pos_chunk))\n",
        "        pos_samples.append(pos_chunk.sample(take, random_state=42))\n",
        "        pos_collected += take\n",
        "\n",
        "    if neg_collected < target_per_class and len(neg_chunk) > 0:\n",
        "        take = min(target_per_class - neg_collected, len(neg_chunk))\n",
        "        neg_samples.append(neg_chunk.sample(take, random_state=42))\n",
        "        neg_collected += take\n",
        "\n",
        "    if pos_collected >= target_per_class and neg_collected >= target_per_class:\n",
        "        break\n",
        "\n",
        "# ---------------- COMBINE & SHUFFLE ----------------\n",
        "df_balanced = pd.concat(pos_samples + neg_samples).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# ---------------- ENCODE TIME ----------------\n",
        "df_balanced['hour'] = df_balanced['Time (UTC)'].str[:2].astype(int)\n",
        "df_balanced['minute'] = df_balanced['Time (UTC)'].str[3:5].astype(int)\n",
        "df_balanced['time_sin'] = np.sin(2 * np.pi * (df_balanced['hour']*60 + df_balanced['minute']) / 1440)\n",
        "df_balanced['time_cos'] = np.cos(2 * np.pi * (df_balanced['hour']*60 + df_balanced['minute']) / 1440)\n",
        "\n",
        "# Drop original Date/Time columns\n",
        "df_balanced.drop(['Date', 'Time (UTC)', 'hour', 'minute'], axis=1, inplace=True)\n",
        "\n",
        "# ---------------- SAVE ----------------\n",
        "df_balanced.to_csv(output_csv, index=False)\n",
        "print(f\"[INFO] Cleaned and balanced dataset saved: {output_csv}\")\n",
        "print(df_balanced.head())\n",
        "print(df_balanced[label_col].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMHPJOJeei2P",
        "outputId": "b3b71a39-87c1-4fe5-da2d-ca717a5887a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Cleaned and balanced dataset saved: /content/drive/MyDrive/AUV Implenation/clean_balanced_10M.csv\n",
            "   Salinity (ppt)  Temperature (°C)  Depth (m)  Sound speed (m/s)  label  \\\n",
            "0       33.026302         10.598071  42.568523        1488.410034      1   \n",
            "1       33.760000         12.189000  38.450000        1494.760000      0   \n",
            "2       32.490000          9.900000   7.680000        1446.480000      0   \n",
            "3       34.800799         10.013184  20.609443        1490.846722      1   \n",
            "4       31.630000         10.988000  10.390000        1493.070000      0   \n",
            "\n",
            "   time_sin  time_cos  \n",
            "0  -0.97237 -0.233445  \n",
            "1   0.95882 -0.284015  \n",
            "2   0.95882 -0.284015  \n",
            "3  -0.97237 -0.233445  \n",
            "4   0.95502 -0.296542  \n",
            "label\n",
            "1    5000000\n",
            "0    5000000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/AUV Implenation/Deep Learning.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhtjD5crcYUr",
        "outputId": "1c567f67-77f7-4be6-b735-176f9c656e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using device: cuda\n",
            "[INFO] Dataset loaded: 10000000 rows\n",
            "[INFO] Data moved to GPU: torch.Size([10000000, 6])\n",
            "\n",
            "[INFO] Epoch 1/40\n",
            "[INFO] Epoch 1 Train Loss=0.6564, Val Loss=0.6318, Train Acc=0.5659, Val Acc=0.5886\n",
            "\n",
            "[INFO] Epoch 2/40\n",
            "[INFO] Epoch 2 Train Loss=0.6321, Val Loss=0.6278, Train Acc=0.5890, Val Acc=0.5916\n",
            "\n",
            "[INFO] Epoch 3/40\n",
            "[INFO] Epoch 3 Train Loss=0.6254, Val Loss=0.6197, Train Acc=0.5998, Val Acc=0.6060\n",
            "\n",
            "[INFO] Epoch 4/40\n",
            "[INFO] Epoch 4 Train Loss=0.6196, Val Loss=0.6144, Train Acc=0.6071, Val Acc=0.6108\n",
            "\n",
            "[INFO] Epoch 5/40\n",
            "[INFO] Epoch 5 Train Loss=0.6146, Val Loss=0.6015, Train Acc=0.6126, Val Acc=0.6277\n",
            "\n",
            "[INFO] Epoch 6/40\n",
            "[INFO] Epoch 6 Train Loss=0.6081, Val Loss=0.5934, Train Acc=0.6203, Val Acc=0.6341\n",
            "\n",
            "[INFO] Epoch 7/40\n",
            "[INFO] Epoch 7 Train Loss=0.6029, Val Loss=0.5903, Train Acc=0.6259, Val Acc=0.6397\n",
            "\n",
            "[INFO] Epoch 8/40\n",
            "[INFO] Epoch 8 Train Loss=0.5988, Val Loss=0.5842, Train Acc=0.6302, Val Acc=0.6431\n",
            "\n",
            "[INFO] Epoch 9/40\n",
            "[INFO] Epoch 9 Train Loss=0.5954, Val Loss=0.5809, Train Acc=0.6333, Val Acc=0.6442\n",
            "\n",
            "[INFO] Epoch 10/40\n",
            "[INFO] Epoch 10 Train Loss=0.5926, Val Loss=0.5807, Train Acc=0.6360, Val Acc=0.6435\n",
            "\n",
            "[INFO] Epoch 11/40\n",
            "[INFO] Epoch 11 Train Loss=0.5901, Val Loss=0.5753, Train Acc=0.6378, Val Acc=0.6480\n",
            "\n",
            "[INFO] Epoch 12/40\n",
            "[INFO] Epoch 12 Train Loss=0.5877, Val Loss=0.5735, Train Acc=0.6398, Val Acc=0.6500\n",
            "\n",
            "[INFO] Epoch 13/40\n",
            "[INFO] Epoch 13 Train Loss=0.5857, Val Loss=0.5719, Train Acc=0.6411, Val Acc=0.6504\n",
            "\n",
            "[INFO] Epoch 14/40\n",
            "[INFO] Epoch 14 Train Loss=0.5840, Val Loss=0.5708, Train Acc=0.6423, Val Acc=0.6515\n",
            "\n",
            "[INFO] Epoch 15/40\n",
            "[INFO] Epoch 15 Train Loss=0.5824, Val Loss=0.5698, Train Acc=0.6437, Val Acc=0.6534\n",
            "\n",
            "[INFO] Epoch 16/40\n",
            "[INFO] Epoch 16 Train Loss=0.5809, Val Loss=0.5681, Train Acc=0.6448, Val Acc=0.6549\n",
            "\n",
            "[INFO] Epoch 17/40\n",
            "[INFO] Epoch 17 Train Loss=0.5797, Val Loss=0.5664, Train Acc=0.6457, Val Acc=0.6574\n",
            "\n",
            "[INFO] Epoch 18/40\n",
            "[INFO] Epoch 18 Train Loss=0.5785, Val Loss=0.5657, Train Acc=0.6468, Val Acc=0.6566\n",
            "\n",
            "[INFO] Epoch 19/40\n",
            "[INFO] Epoch 19 Train Loss=0.5773, Val Loss=0.5645, Train Acc=0.6476, Val Acc=0.6590\n",
            "\n",
            "[INFO] Epoch 20/40\n",
            "[INFO] Epoch 20 Train Loss=0.5764, Val Loss=0.5647, Train Acc=0.6483, Val Acc=0.6587\n",
            "\n",
            "[INFO] Epoch 21/40\n",
            "[INFO] Epoch 21 Train Loss=0.5756, Val Loss=0.5621, Train Acc=0.6490, Val Acc=0.6607\n",
            "\n",
            "[INFO] Epoch 22/40\n",
            "[INFO] Epoch 22 Train Loss=0.5746, Val Loss=0.5626, Train Acc=0.6497, Val Acc=0.6599\n",
            "\n",
            "[INFO] Epoch 23/40\n",
            "[INFO] Epoch 23 Train Loss=0.5739, Val Loss=0.5620, Train Acc=0.6501, Val Acc=0.6608\n",
            "\n",
            "[INFO] Epoch 24/40\n",
            "[INFO] Epoch 24 Train Loss=0.5732, Val Loss=0.5609, Train Acc=0.6508, Val Acc=0.6621\n",
            "\n",
            "[INFO] Epoch 25/40\n",
            "[INFO] Epoch 25 Train Loss=0.5726, Val Loss=0.5606, Train Acc=0.6511, Val Acc=0.6617\n",
            "\n",
            "[INFO] Epoch 26/40\n",
            "[INFO] Epoch 26 Train Loss=0.5721, Val Loss=0.5602, Train Acc=0.6515, Val Acc=0.6624\n",
            "\n",
            "[INFO] Epoch 27/40\n",
            "[INFO] Epoch 27 Train Loss=0.5716, Val Loss=0.5596, Train Acc=0.6519, Val Acc=0.6626\n",
            "\n",
            "[INFO] Epoch 28/40\n",
            "[INFO] Epoch 28 Train Loss=0.5712, Val Loss=0.5595, Train Acc=0.6522, Val Acc=0.6629\n",
            "\n",
            "[INFO] Epoch 29/40\n",
            "[INFO] Epoch 29 Train Loss=0.5708, Val Loss=0.5590, Train Acc=0.6525, Val Acc=0.6631\n",
            "\n",
            "[INFO] Epoch 30/40\n",
            "[INFO] Epoch 30 Train Loss=0.5705, Val Loss=0.5590, Train Acc=0.6529, Val Acc=0.6631\n",
            "\n",
            "[INFO] Epoch 31/40\n",
            "[INFO] Epoch 31 Train Loss=0.5703, Val Loss=0.5585, Train Acc=0.6530, Val Acc=0.6634\n",
            "\n",
            "[INFO] Epoch 32/40\n",
            "[INFO] Epoch 32 Train Loss=0.5699, Val Loss=0.5584, Train Acc=0.6532, Val Acc=0.6635\n",
            "\n",
            "[INFO] Epoch 33/40\n",
            "[INFO] Epoch 33 Train Loss=0.5697, Val Loss=0.5579, Train Acc=0.6534, Val Acc=0.6640\n",
            "\n",
            "[INFO] Epoch 34/40\n",
            "[INFO] Epoch 34 Train Loss=0.5696, Val Loss=0.5584, Train Acc=0.6535, Val Acc=0.6634\n",
            "\n",
            "[INFO] Epoch 35/40\n",
            "[INFO] Epoch 35 Train Loss=0.5696, Val Loss=0.5585, Train Acc=0.6535, Val Acc=0.6633\n",
            "\n",
            "[INFO] Epoch 36/40\n",
            "[INFO] Epoch 36 Train Loss=0.5694, Val Loss=0.5581, Train Acc=0.6534, Val Acc=0.6636\n",
            "\n",
            "[INFO] Epoch 37/40\n",
            "[INFO] Epoch 37 Train Loss=0.5694, Val Loss=0.5576, Train Acc=0.6534, Val Acc=0.6640\n",
            "\n",
            "[INFO] Epoch 38/40\n",
            "[INFO] Epoch 38 Train Loss=0.5693, Val Loss=0.5579, Train Acc=0.6534, Val Acc=0.6638\n",
            "[INFO] Early stopping triggered at epoch 38\n",
            "[INFO] Best validation accuracy: 0.6640\n",
            "[INFO] Training finished. Metrics saved in /content/drive/MyDrive/AUV Implenation/results_full_gpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Teacher model definition\n",
        "class TeacherNet(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(TeacherNet, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Example usage\n",
        "input_dim = 100   # put your actual input dim\n",
        "num_classes = 10  # put your actual num classes\n",
        "teacher = TeacherNet(input_dim, num_classes)\n",
        "\n",
        "print(\"Teacher architecture:\")\n",
        "print(teacher)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8_KXXKD3GfE",
        "outputId": "1fa3d581-2487-4a0d-b581-78de8d9832cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher architecture:\n",
            "TeacherNet(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Student model definition\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(StudentNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "input_dim = 100   # put your actual input dim\n",
        "num_classes = 10  # put your actual num classes\n",
        "student = StudentNet(input_dim, num_classes)\n",
        "\n",
        "print(\"Student architecture:\")\n",
        "print(student)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9WZIHCC3J-G",
        "outputId": "3c57a7f1-67e9-4f93-fe13-ef0690f9e693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student architecture:\n",
            "StudentNet(\n",
            "  (fc1): Linear(in_features=100, out_features=32, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# ---------- Config ----------\n",
        "teacher_path = \"/content/drive/MyDrive/AUV Implenation/results_full_gpu/best_teacher.pth\"\n",
        "student_path = \"/content/drive/MyDrive/AUV Implenation/final_model_gpu.pt\"   # if exists, else skip\n",
        "\n",
        "# ---------- Dummy Dataset Check ----------\n",
        "def verify_dataset(dataset):\n",
        "    first_x, first_y = dataset[0]\n",
        "    input_dim = first_x.shape[0] if len(first_x.shape) == 1 else first_x.numel()\n",
        "    num_classes = len(set([y.item() for _, y in dataset]))\n",
        "    print(f\"[DATASET] input_dim={input_dim}, num_classes={num_classes}\")\n",
        "    return input_dim, num_classes\n",
        "\n",
        "# ---------- Model Check ----------\n",
        "def check_checkpoint(path):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"[MISSING] {path} not found.\")\n",
        "        return None\n",
        "    ckpt = torch.load(path, map_location=\"cpu\")\n",
        "    print(f\"[FOUND] {path} ✅\")\n",
        "    print(f\"   Keys in checkpoint: {list(ckpt.keys())[:10]} ...\")\n",
        "    return ckpt\n",
        "\n",
        "# Example usage:\n",
        "# Replace `train_dataset` with your dataset object\n",
        "# input_dim, num_classes = verify_dataset(train_dataset)\n",
        "\n",
        "teacher_ckpt = check_checkpoint(teacher_path)\n",
        "student_ckpt = check_checkpoint(student_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6V5mYM74_fn",
        "outputId": "bceaf8f3-791d-422c-83d8-14f4d2498511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOUND] /content/drive/MyDrive/AUV Implenation/results_full_gpu/best_teacher.pth ✅\n",
            "   Keys in checkpoint: ['layers.0.weight', 'layers.0.bias', 'layers.1.weight', 'layers.1.bias', 'layers.4.weight', 'layers.4.bias', 'layers.5.weight', 'layers.5.bias', 'layers.8.weight', 'layers.8.bias'] ...\n",
            "[FOUND] /content/drive/MyDrive/AUV Implenation/final_model_gpu.pt ✅\n",
            "   Keys in checkpoint: ['model_state_dict', 'features', 'classes', 'training_time', 'device', 'rounds'] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------------------\n",
        "# Load real dataset\n",
        "# -------------------------------\n",
        "DATA_PATH = \"/content/drive/MyDrive/AUV Implenation/balanced_sample_200MB.csv\"   # replace with actual path\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"[INFO] Loaded dataset:\", DATA_PATH)\n",
        "print(\"[INFO] Shape:\", df.shape)\n",
        "print(\"[INFO] Columns:\", df.columns.tolist())\n",
        "\n",
        "# -------------------------------\n",
        "# Feature & Target Split\n",
        "# -------------------------------\n",
        "# Example: assuming last column is the label\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "print(\"[INFO] Features shape:\", X.shape)\n",
        "print(\"[INFO] Labels shape:\", y.shape)\n",
        "\n",
        "# -------------------------------\n",
        "# Input dimension & Num classes\n",
        "# -------------------------------\n",
        "input_dim = X.shape[1]\n",
        "num_classes = len(set(y))\n",
        "\n",
        "print(\"[INFO] input_dim =\", input_dim)\n",
        "print(\"[INFO] num_classes =\", num_classes)\n",
        "\n",
        "# -------------------------------\n",
        "# Convert to torch tensors\n",
        "# -------------------------------\n",
        "X_train = torch.tensor(X, dtype=torch.float32)\n",
        "y_train = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "print(\"[INFO] Torch tensors created\")\n",
        "print(\"    X_train:\", X_train.shape)\n",
        "print(\"    y_train:\", y_train.shape)\n",
        "print(\"    Classes:\", torch.unique(y_train))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL8k9XJT7FfE",
        "outputId": "3fb3b630-3831-4bf3-bf8e-696c76c173d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loaded dataset: /content/drive/MyDrive/AUV Implenation/balanced_sample_200MB.csv\n",
            "[INFO] Shape: (5225634, 5)\n",
            "[INFO] Columns: ['Salinity (ppt)', 'Temperature (°C)', 'Depth (m)', 'Sound speed (m/s)', 'label']\n",
            "[INFO] Features shape: (5225634, 4)\n",
            "[INFO] Labels shape: (5225634,)\n",
            "[INFO] input_dim = 4\n",
            "[INFO] num_classes = 2\n",
            "[INFO] Torch tensors created\n",
            "    X_train: torch.Size([5225634, 4])\n",
            "    y_train: torch.Size([5225634])\n",
            "    Classes: tensor([0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# KD from 6D Teacher -> 4D Student (Colab-ready)\n",
        "# =========================\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -------------------------------\n",
        "# Config\n",
        "# -------------------------------\n",
        "DATA_PATH = \"/content/drive/MyDrive/AUV Implenation/balanced_sample_200MB.csv\"\n",
        "TEACHER_PATH = \"/content/drive/MyDrive/AUV Implenation/results_full_gpu/best_teacher.pth\"\n",
        "STUDENT_SAVE_PATH = \"/content/drive/MyDrive/AUV Implenation/student_kd.pth\"\n",
        "METRICS_CSV = \"/content/drive/MyDrive/AUV Implenation/student_kd_metrics.csv\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 8192               # adjust if you hit OOM\n",
        "EPOCHS = 50\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "ALPHA = 0.7                     # KD weight\n",
        "TEMP = 2.0                      # KD temperature\n",
        "VAL_FRAC = 0.2\n",
        "SEED = 42\n",
        "NUM_WORKERS = 0                 # 0 safest in Colab\n",
        "PIN_MEMORY = True if DEVICE.type == \"cuda\" else False\n",
        "PRINT_EVERY = 100\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(f\"[INFO] Device: {DEVICE}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Teacher & Student Architectures\n",
        "# -------------------------------\n",
        "class TeacherDeep(nn.Module):\n",
        "    def __init__(self, input_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        hidden = [512, 256, 128, 64]\n",
        "        layers = []\n",
        "        prev = input_dim\n",
        "        for h in hidden:\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            layers.append(nn.LayerNorm(h))\n",
        "            layers.append(nn.LeakyReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "            prev = h\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.out = nn.Linear(prev, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.out(self.layers(x))\n",
        "\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# -------------------------------\n",
        "# KD Loss\n",
        "# -------------------------------\n",
        "def kd_loss_fn(student_logits, teacher_logits, targets, T=2.0, alpha=0.7):\n",
        "    ce = F.cross_entropy(student_logits, targets)\n",
        "    kl = F.kl_div(\n",
        "        F.log_softmax(student_logits / T, dim=1),\n",
        "        F.softmax(teacher_logits / T, dim=1),\n",
        "        reduction=\"batchmean\"\n",
        "    ) * (T * T)\n",
        "    return alpha * kl + (1.0 - alpha) * ce\n",
        "\n",
        "# -------------------------------\n",
        "# Load Dataset\n",
        "# -------------------------------\n",
        "print(\"[INFO] Loading dataset...\")\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
        "\n",
        "X = df.iloc[:, :-1].values.astype(np.float32)\n",
        "y = df.iloc[:, -1].values.astype(np.int64)\n",
        "num_classes = int(len(np.unique(y)))\n",
        "input_dim = X.shape[1]\n",
        "print(f\"[INFO] Data shape X={X.shape}, y={y.shape}, input_dim={input_dim}, num_classes={num_classes}\")\n",
        "\n",
        "X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(\n",
        "    X, y, test_size=VAL_FRAC, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize\n",
        "mean_ = X_train_np.mean(axis=0, keepdims=True)\n",
        "std_  = X_train_np.std(axis=0, keepdims=True)\n",
        "std_[std_ == 0.0] = 1.0\n",
        "X_train_np = (X_train_np - mean_) / std_\n",
        "X_val_np   = (X_val_np   - mean_) / std_\n",
        "\n",
        "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train_np, dtype=torch.long)\n",
        "X_val   = torch.tensor(X_val_np, dtype=torch.float32)\n",
        "y_val   = torch.tensor(y_val_np, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    TensorDataset(X_train, y_train),\n",
        "    batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    TensorDataset(X_val, y_val),\n",
        "    batch_size=BATCH_SIZE*2, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Teacher (6D input)\n",
        "# -------------------------------\n",
        "teacher_input_dim = 6\n",
        "teacher = TeacherDeep(teacher_input_dim, num_classes).to(DEVICE)\n",
        "\n",
        "print(\"[INFO] Loading teacher checkpoint...\")\n",
        "sd = torch.load(TEACHER_PATH, map_location=\"cpu\")\n",
        "if isinstance(sd, dict) and \"state_dict\" in sd:\n",
        "    sd = sd[\"state_dict\"]\n",
        "teacher.load_state_dict(sd, strict=True)\n",
        "teacher.eval()\n",
        "\n",
        "# -------------------------------\n",
        "# Student\n",
        "# -------------------------------\n",
        "student_hidden = 32\n",
        "student = StudentNet(input_dim=input_dim, hidden_dim=student_hidden, num_classes=num_classes).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(student.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "# -------------------------------\n",
        "# Eval helper\n",
        "# -------------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for xb, yb in data_loader:\n",
        "        xb = xb.to(DEVICE, non_blocking=True)\n",
        "        yb = yb.to(DEVICE, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb, reduction=\"sum\")\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "        loss_sum += loss.item()\n",
        "    acc = correct / max(1, total)\n",
        "    return loss_sum / max(1, total), acc\n",
        "\n",
        "# -------------------------------\n",
        "# Pre-KD eval\n",
        "# -------------------------------\n",
        "pre_loss, pre_acc = evaluate(student, val_loader)\n",
        "print(f\"[PRE-KD] Val Loss={pre_loss:.4f}, Val Acc={pre_acc:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Train KD\n",
        "# -------------------------------\n",
        "def train_kd(epochs=EPOCHS):\n",
        "    metrics = []\n",
        "    for epoch in range(1, epochs+1):\n",
        "        student.train()\n",
        "        t0 = time.time()\n",
        "        running = 0.0\n",
        "        for i, (xb, yb) in enumerate(train_loader, 1):\n",
        "            xb = xb.to(DEVICE, non_blocking=True)\n",
        "            yb = yb.to(DEVICE, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(DEVICE.type == \"cuda\")):\n",
        "                s_logits = student(xb)\n",
        "                # pad zeros for teacher\n",
        "                pad = torch.zeros(xb.size(0), 2, device=xb.device, dtype=xb.dtype)\n",
        "                xb_teacher = torch.cat([xb, pad], dim=1)\n",
        "                with torch.no_grad():\n",
        "                    t_logits = teacher(xb_teacher)\n",
        "                loss = kd_loss_fn(s_logits, t_logits, yb, T=TEMP, alpha=ALPHA)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            running += loss.item()\n",
        "            if i % PRINT_EVERY == 0:\n",
        "                print(f\"[Epoch {epoch}] Batch {i}/{len(train_loader)} KD-Loss={running/PRINT_EVERY:.4f}\")\n",
        "                running = 0.0\n",
        "        val_loss, val_acc = evaluate(student, val_loader)\n",
        "        dt = time.time() - t0\n",
        "        print(f\"[Epoch {epoch}] Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}  (epoch time {dt:.1f}s)\")\n",
        "        metrics.append({\"epoch\": epoch, \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
        "    return metrics\n",
        "\n",
        "metrics = train_kd(EPOCHS)\n",
        "\n",
        "# -------------------------------\n",
        "# Post-KD eval & save\n",
        "# -------------------------------\n",
        "post_loss, post_acc = evaluate(student, val_loader)\n",
        "print(f\"[POST-KD] Val Loss={post_loss:.4f}, Val Acc={post_acc:.4f}\")\n",
        "\n",
        "os.makedirs(os.path.dirname(STUDENT_SAVE_PATH), exist_ok=True)\n",
        "torch.save({\n",
        "    \"state_dict\": student.state_dict(),\n",
        "    \"input_dim\": input_dim,\n",
        "    \"num_classes\": num_classes,\n",
        "    \"standardize_mean\": mean_,\n",
        "    \"standardize_std\": std_,\n",
        "    \"kd\": {\"alpha\": ALPHA, \"temperature\": TEMP, \"epochs\": EPOCHS},\n",
        "}, STUDENT_SAVE_PATH)\n",
        "print(f\"[INFO] KD student saved -> {STUDENT_SAVE_PATH}\")\n",
        "\n",
        "rows = [{\"phase\":\"pre_kd\",\"val_loss\":pre_loss,\"val_acc\":pre_acc}]\n",
        "rows += [{\"phase\":f\"epoch_{m['epoch']}\",\"val_loss\":m[\"val_loss\"],\"val_acc\":m[\"val_acc\"]} for m in metrics]\n",
        "rows += [{\"phase\":\"post_kd\",\"val_loss\":post_loss,\"val_acc\":post_acc}]\n",
        "pd.DataFrame(rows).to_csv(METRICS_CSV, index=False)\n",
        "print(f\"[INFO] Metrics saved -> {METRICS_CSV}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm38L3eh21He",
        "outputId": "c089b956-bd0a-472b-b637-e0f93bad65ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Device: cuda\n",
            "[INFO] Loading dataset...\n",
            "[INFO] Data shape X=(5225634, 4), y=(5225634,), input_dim=4, num_classes=2\n",
            "[INFO] Loading teacher checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4287465235.py:144: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == \"cuda\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PRE-KD] Val Loss=0.7167, Val Acc=0.4580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4287465235.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(DEVICE.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Batch 100/511 KD-Loss=0.3819\n",
            "[Epoch 1] Batch 200/511 KD-Loss=0.3117\n",
            "[Epoch 1] Batch 300/511 KD-Loss=0.2832\n",
            "[Epoch 1] Batch 400/511 KD-Loss=0.2694\n",
            "[Epoch 1] Batch 500/511 KD-Loss=0.2612\n",
            "[Epoch 1] Val Loss=0.6343, Val Acc=0.5755  (epoch time 65.6s)\n",
            "[Epoch 2] Batch 100/511 KD-Loss=0.2557\n",
            "[Epoch 2] Batch 200/511 KD-Loss=0.2523\n",
            "[Epoch 2] Batch 300/511 KD-Loss=0.2496\n",
            "[Epoch 2] Batch 400/511 KD-Loss=0.2481\n",
            "[Epoch 2] Batch 500/511 KD-Loss=0.2459\n",
            "[Epoch 2] Val Loss=0.6277, Val Acc=0.5834  (epoch time 65.3s)\n",
            "[Epoch 3] Batch 100/511 KD-Loss=0.2445\n",
            "[Epoch 3] Batch 200/511 KD-Loss=0.2429\n",
            "[Epoch 3] Batch 300/511 KD-Loss=0.2420\n",
            "[Epoch 3] Batch 400/511 KD-Loss=0.2406\n",
            "[Epoch 3] Batch 500/511 KD-Loss=0.2389\n",
            "[Epoch 3] Val Loss=0.6240, Val Acc=0.5829  (epoch time 65.3s)\n",
            "[Epoch 4] Batch 100/511 KD-Loss=0.2377\n",
            "[Epoch 4] Batch 200/511 KD-Loss=0.2370\n",
            "[Epoch 4] Batch 300/511 KD-Loss=0.2356\n",
            "[Epoch 4] Batch 400/511 KD-Loss=0.2350\n",
            "[Epoch 4] Batch 500/511 KD-Loss=0.2345\n",
            "[Epoch 4] Val Loss=0.6215, Val Acc=0.5837  (epoch time 65.3s)\n",
            "[Epoch 5] Batch 100/511 KD-Loss=0.2335\n",
            "[Epoch 5] Batch 200/511 KD-Loss=0.2329\n",
            "[Epoch 5] Batch 300/511 KD-Loss=0.2317\n",
            "[Epoch 5] Batch 400/511 KD-Loss=0.2312\n",
            "[Epoch 5] Batch 500/511 KD-Loss=0.2296\n",
            "[Epoch 5] Val Loss=0.6193, Val Acc=0.5855  (epoch time 65.5s)\n",
            "[Epoch 6] Batch 100/511 KD-Loss=0.2279\n",
            "[Epoch 6] Batch 200/511 KD-Loss=0.2265\n",
            "[Epoch 6] Batch 300/511 KD-Loss=0.2249\n",
            "[Epoch 6] Batch 400/511 KD-Loss=0.2235\n",
            "[Epoch 6] Batch 500/511 KD-Loss=0.2223\n",
            "[Epoch 6] Val Loss=0.6172, Val Acc=0.5885  (epoch time 65.7s)\n",
            "[Epoch 7] Batch 100/511 KD-Loss=0.2207\n",
            "[Epoch 7] Batch 200/511 KD-Loss=0.2194\n",
            "[Epoch 7] Batch 300/511 KD-Loss=0.2180\n",
            "[Epoch 7] Batch 400/511 KD-Loss=0.2167\n",
            "[Epoch 7] Batch 500/511 KD-Loss=0.2151\n",
            "[Epoch 7] Val Loss=0.6137, Val Acc=0.5966  (epoch time 65.6s)\n",
            "[Epoch 8] Batch 100/511 KD-Loss=0.2138\n",
            "[Epoch 8] Batch 200/511 KD-Loss=0.2128\n",
            "[Epoch 8] Batch 300/511 KD-Loss=0.2115\n",
            "[Epoch 8] Batch 400/511 KD-Loss=0.2104\n",
            "[Epoch 8] Batch 500/511 KD-Loss=0.2096\n",
            "[Epoch 8] Val Loss=0.6101, Val Acc=0.6063  (epoch time 65.4s)\n",
            "[Epoch 9] Batch 100/511 KD-Loss=0.2080\n",
            "[Epoch 9] Batch 200/511 KD-Loss=0.2074\n",
            "[Epoch 9] Batch 300/511 KD-Loss=0.2064\n",
            "[Epoch 9] Batch 400/511 KD-Loss=0.2054\n",
            "[Epoch 9] Batch 500/511 KD-Loss=0.2046\n",
            "[Epoch 9] Val Loss=0.6083, Val Acc=0.6093  (epoch time 65.4s)\n",
            "[Epoch 10] Batch 100/511 KD-Loss=0.2038\n",
            "[Epoch 10] Batch 200/511 KD-Loss=0.2030\n",
            "[Epoch 10] Batch 300/511 KD-Loss=0.2022\n",
            "[Epoch 10] Batch 400/511 KD-Loss=0.2017\n",
            "[Epoch 10] Batch 500/511 KD-Loss=0.2010\n",
            "[Epoch 10] Val Loss=0.6064, Val Acc=0.6120  (epoch time 65.3s)\n",
            "[Epoch 11] Batch 100/511 KD-Loss=0.2004\n",
            "[Epoch 11] Batch 200/511 KD-Loss=0.1997\n",
            "[Epoch 11] Batch 300/511 KD-Loss=0.1993\n",
            "[Epoch 11] Batch 400/511 KD-Loss=0.1988\n",
            "[Epoch 11] Batch 500/511 KD-Loss=0.1983\n",
            "[Epoch 11] Val Loss=0.6060, Val Acc=0.6118  (epoch time 65.4s)\n",
            "[Epoch 12] Batch 100/511 KD-Loss=0.1978\n",
            "[Epoch 12] Batch 200/511 KD-Loss=0.1974\n",
            "[Epoch 12] Batch 300/511 KD-Loss=0.1970\n",
            "[Epoch 12] Batch 400/511 KD-Loss=0.1966\n",
            "[Epoch 12] Batch 500/511 KD-Loss=0.1963\n",
            "[Epoch 12] Val Loss=0.6053, Val Acc=0.6128  (epoch time 65.3s)\n",
            "[Epoch 13] Batch 100/511 KD-Loss=0.1959\n",
            "[Epoch 13] Batch 200/511 KD-Loss=0.1956\n",
            "[Epoch 13] Batch 300/511 KD-Loss=0.1953\n",
            "[Epoch 13] Batch 400/511 KD-Loss=0.1951\n",
            "[Epoch 13] Batch 500/511 KD-Loss=0.1949\n",
            "[Epoch 13] Val Loss=0.6041, Val Acc=0.6134  (epoch time 65.4s)\n",
            "[Epoch 14] Batch 100/511 KD-Loss=0.1945\n",
            "[Epoch 14] Batch 200/511 KD-Loss=0.1944\n",
            "[Epoch 14] Batch 300/511 KD-Loss=0.1942\n",
            "[Epoch 14] Batch 400/511 KD-Loss=0.1941\n",
            "[Epoch 14] Batch 500/511 KD-Loss=0.1939\n",
            "[Epoch 14] Val Loss=0.6041, Val Acc=0.6125  (epoch time 65.4s)\n",
            "[Epoch 15] Batch 100/511 KD-Loss=0.1938\n",
            "[Epoch 15] Batch 200/511 KD-Loss=0.1935\n",
            "[Epoch 15] Batch 300/511 KD-Loss=0.1933\n",
            "[Epoch 15] Batch 400/511 KD-Loss=0.1933\n",
            "[Epoch 15] Batch 500/511 KD-Loss=0.1932\n",
            "[Epoch 15] Val Loss=0.6030, Val Acc=0.6135  (epoch time 65.3s)\n",
            "[Epoch 16] Batch 100/511 KD-Loss=0.1931\n",
            "[Epoch 16] Batch 200/511 KD-Loss=0.1930\n",
            "[Epoch 16] Batch 300/511 KD-Loss=0.1928\n",
            "[Epoch 16] Batch 400/511 KD-Loss=0.1928\n",
            "[Epoch 16] Batch 500/511 KD-Loss=0.1924\n",
            "[Epoch 16] Val Loss=0.6021, Val Acc=0.6141  (epoch time 65.4s)\n",
            "[Epoch 17] Batch 100/511 KD-Loss=0.1926\n",
            "[Epoch 17] Batch 200/511 KD-Loss=0.1925\n",
            "[Epoch 17] Batch 300/511 KD-Loss=0.1925\n",
            "[Epoch 17] Batch 400/511 KD-Loss=0.1923\n",
            "[Epoch 17] Batch 500/511 KD-Loss=0.1920\n",
            "[Epoch 17] Val Loss=0.6016, Val Acc=0.6145  (epoch time 65.3s)\n",
            "[Epoch 18] Batch 100/511 KD-Loss=0.1922\n",
            "[Epoch 18] Batch 200/511 KD-Loss=0.1922\n",
            "[Epoch 18] Batch 300/511 KD-Loss=0.1919\n",
            "[Epoch 18] Batch 400/511 KD-Loss=0.1921\n",
            "[Epoch 18] Batch 500/511 KD-Loss=0.1920\n",
            "[Epoch 18] Val Loss=0.6015, Val Acc=0.6142  (epoch time 65.4s)\n",
            "[Epoch 19] Batch 100/511 KD-Loss=0.1922\n",
            "[Epoch 19] Batch 200/511 KD-Loss=0.1918\n",
            "[Epoch 19] Batch 300/511 KD-Loss=0.1917\n",
            "[Epoch 19] Batch 400/511 KD-Loss=0.1918\n",
            "[Epoch 19] Batch 500/511 KD-Loss=0.1917\n",
            "[Epoch 19] Val Loss=0.6011, Val Acc=0.6148  (epoch time 66.2s)\n",
            "[Epoch 20] Batch 100/511 KD-Loss=0.1917\n",
            "[Epoch 20] Batch 200/511 KD-Loss=0.1918\n",
            "[Epoch 20] Batch 300/511 KD-Loss=0.1917\n",
            "[Epoch 20] Batch 400/511 KD-Loss=0.1915\n",
            "[Epoch 20] Batch 500/511 KD-Loss=0.1916\n",
            "[Epoch 20] Val Loss=0.5999, Val Acc=0.6154  (epoch time 67.2s)\n",
            "[Epoch 21] Batch 100/511 KD-Loss=0.1914\n",
            "[Epoch 21] Batch 200/511 KD-Loss=0.1915\n",
            "[Epoch 21] Batch 300/511 KD-Loss=0.1915\n",
            "[Epoch 21] Batch 400/511 KD-Loss=0.1915\n",
            "[Epoch 21] Batch 500/511 KD-Loss=0.1916\n",
            "[Epoch 21] Val Loss=0.6003, Val Acc=0.6162  (epoch time 67.2s)\n",
            "[Epoch 22] Batch 100/511 KD-Loss=0.1914\n",
            "[Epoch 22] Batch 200/511 KD-Loss=0.1913\n",
            "[Epoch 22] Batch 300/511 KD-Loss=0.1912\n",
            "[Epoch 22] Batch 400/511 KD-Loss=0.1913\n",
            "[Epoch 22] Batch 500/511 KD-Loss=0.1913\n",
            "[Epoch 22] Val Loss=0.5997, Val Acc=0.6180  (epoch time 66.7s)\n",
            "[Epoch 23] Batch 100/511 KD-Loss=0.1912\n",
            "[Epoch 23] Batch 200/511 KD-Loss=0.1912\n",
            "[Epoch 23] Batch 300/511 KD-Loss=0.1911\n",
            "[Epoch 23] Batch 400/511 KD-Loss=0.1911\n",
            "[Epoch 23] Batch 500/511 KD-Loss=0.1910\n",
            "[Epoch 23] Val Loss=0.5994, Val Acc=0.6184  (epoch time 66.3s)\n",
            "[Epoch 24] Batch 100/511 KD-Loss=0.1909\n",
            "[Epoch 24] Batch 200/511 KD-Loss=0.1909\n",
            "[Epoch 24] Batch 300/511 KD-Loss=0.1906\n",
            "[Epoch 24] Batch 400/511 KD-Loss=0.1907\n",
            "[Epoch 24] Batch 500/511 KD-Loss=0.1907\n",
            "[Epoch 24] Val Loss=0.5991, Val Acc=0.6200  (epoch time 66.1s)\n",
            "[Epoch 25] Batch 100/511 KD-Loss=0.1906\n",
            "[Epoch 25] Batch 200/511 KD-Loss=0.1906\n",
            "[Epoch 25] Batch 300/511 KD-Loss=0.1905\n",
            "[Epoch 25] Batch 400/511 KD-Loss=0.1905\n",
            "[Epoch 25] Batch 500/511 KD-Loss=0.1904\n",
            "[Epoch 25] Val Loss=0.5992, Val Acc=0.6234  (epoch time 65.6s)\n",
            "[Epoch 26] Batch 100/511 KD-Loss=0.1904\n",
            "[Epoch 26] Batch 200/511 KD-Loss=0.1905\n",
            "[Epoch 26] Batch 300/511 KD-Loss=0.1904\n",
            "[Epoch 26] Batch 400/511 KD-Loss=0.1903\n",
            "[Epoch 26] Batch 500/511 KD-Loss=0.1904\n",
            "[Epoch 26] Val Loss=0.5987, Val Acc=0.6228  (epoch time 65.6s)\n",
            "[Epoch 27] Batch 100/511 KD-Loss=0.1903\n",
            "[Epoch 27] Batch 200/511 KD-Loss=0.1903\n",
            "[Epoch 27] Batch 300/511 KD-Loss=0.1902\n",
            "[Epoch 27] Batch 400/511 KD-Loss=0.1902\n",
            "[Epoch 27] Batch 500/511 KD-Loss=0.1901\n",
            "[Epoch 27] Val Loss=0.5981, Val Acc=0.6211  (epoch time 65.5s)\n",
            "[Epoch 28] Batch 100/511 KD-Loss=0.1903\n",
            "[Epoch 28] Batch 200/511 KD-Loss=0.1901\n",
            "[Epoch 28] Batch 300/511 KD-Loss=0.1900\n",
            "[Epoch 28] Batch 400/511 KD-Loss=0.1901\n",
            "[Epoch 28] Batch 500/511 KD-Loss=0.1900\n",
            "[Epoch 28] Val Loss=0.5986, Val Acc=0.6215  (epoch time 65.4s)\n",
            "[Epoch 29] Batch 100/511 KD-Loss=0.1901\n",
            "[Epoch 29] Batch 200/511 KD-Loss=0.1901\n",
            "[Epoch 29] Batch 300/511 KD-Loss=0.1901\n",
            "[Epoch 29] Batch 400/511 KD-Loss=0.1901\n",
            "[Epoch 29] Batch 500/511 KD-Loss=0.1898\n",
            "[Epoch 29] Val Loss=0.5989, Val Acc=0.6217  (epoch time 65.3s)\n",
            "[Epoch 30] Batch 100/511 KD-Loss=0.1899\n",
            "[Epoch 30] Batch 200/511 KD-Loss=0.1900\n",
            "[Epoch 30] Batch 300/511 KD-Loss=0.1900\n",
            "[Epoch 30] Batch 400/511 KD-Loss=0.1898\n",
            "[Epoch 30] Batch 500/511 KD-Loss=0.1900\n",
            "[Epoch 30] Val Loss=0.5984, Val Acc=0.6210  (epoch time 65.4s)\n",
            "[Epoch 31] Batch 100/511 KD-Loss=0.1898\n",
            "[Epoch 31] Batch 200/511 KD-Loss=0.1899\n",
            "[Epoch 31] Batch 300/511 KD-Loss=0.1899\n",
            "[Epoch 31] Batch 400/511 KD-Loss=0.1899\n",
            "[Epoch 31] Batch 500/511 KD-Loss=0.1897\n",
            "[Epoch 31] Val Loss=0.5987, Val Acc=0.6196  (epoch time 65.3s)\n",
            "[Epoch 32] Batch 100/511 KD-Loss=0.1899\n",
            "[Epoch 32] Batch 200/511 KD-Loss=0.1899\n",
            "[Epoch 32] Batch 300/511 KD-Loss=0.1896\n",
            "[Epoch 32] Batch 400/511 KD-Loss=0.1897\n",
            "[Epoch 32] Batch 500/511 KD-Loss=0.1897\n",
            "[Epoch 32] Val Loss=0.5989, Val Acc=0.6226  (epoch time 65.3s)\n",
            "[Epoch 33] Batch 100/511 KD-Loss=0.1897\n",
            "[Epoch 33] Batch 200/511 KD-Loss=0.1897\n",
            "[Epoch 33] Batch 300/511 KD-Loss=0.1895\n",
            "[Epoch 33] Batch 400/511 KD-Loss=0.1895\n",
            "[Epoch 33] Batch 500/511 KD-Loss=0.1895\n",
            "[Epoch 33] Val Loss=0.5983, Val Acc=0.6227  (epoch time 65.4s)\n",
            "[Epoch 34] Batch 100/511 KD-Loss=0.1895\n",
            "[Epoch 34] Batch 200/511 KD-Loss=0.1896\n",
            "[Epoch 34] Batch 300/511 KD-Loss=0.1893\n",
            "[Epoch 34] Batch 400/511 KD-Loss=0.1894\n",
            "[Epoch 34] Batch 500/511 KD-Loss=0.1893\n",
            "[Epoch 34] Val Loss=0.5981, Val Acc=0.6210  (epoch time 65.6s)\n",
            "[Epoch 35] Batch 100/511 KD-Loss=0.1891\n",
            "[Epoch 35] Batch 200/511 KD-Loss=0.1892\n",
            "[Epoch 35] Batch 300/511 KD-Loss=0.1890\n",
            "[Epoch 35] Batch 400/511 KD-Loss=0.1888\n",
            "[Epoch 35] Batch 500/511 KD-Loss=0.1886\n",
            "[Epoch 35] Val Loss=0.5984, Val Acc=0.6197  (epoch time 65.5s)\n",
            "[Epoch 36] Batch 100/511 KD-Loss=0.1883\n",
            "[Epoch 36] Batch 200/511 KD-Loss=0.1881\n",
            "[Epoch 36] Batch 300/511 KD-Loss=0.1878\n",
            "[Epoch 36] Batch 400/511 KD-Loss=0.1876\n",
            "[Epoch 36] Batch 500/511 KD-Loss=0.1874\n",
            "[Epoch 36] Val Loss=0.5974, Val Acc=0.6244  (epoch time 65.5s)\n",
            "[Epoch 37] Batch 100/511 KD-Loss=0.1870\n",
            "[Epoch 37] Batch 200/511 KD-Loss=0.1867\n",
            "[Epoch 37] Batch 300/511 KD-Loss=0.1864\n",
            "[Epoch 37] Batch 400/511 KD-Loss=0.1862\n",
            "[Epoch 37] Batch 500/511 KD-Loss=0.1863\n",
            "[Epoch 37] Val Loss=0.5964, Val Acc=0.6236  (epoch time 65.3s)\n",
            "[Epoch 38] Batch 100/511 KD-Loss=0.1862\n",
            "[Epoch 38] Batch 200/511 KD-Loss=0.1859\n",
            "[Epoch 38] Batch 300/511 KD-Loss=0.1856\n",
            "[Epoch 38] Batch 400/511 KD-Loss=0.1858\n",
            "[Epoch 38] Batch 500/511 KD-Loss=0.1857\n",
            "[Epoch 38] Val Loss=0.5963, Val Acc=0.6238  (epoch time 65.5s)\n",
            "[Epoch 39] Batch 100/511 KD-Loss=0.1855\n",
            "[Epoch 39] Batch 200/511 KD-Loss=0.1855\n",
            "[Epoch 39] Batch 300/511 KD-Loss=0.1853\n",
            "[Epoch 39] Batch 400/511 KD-Loss=0.1855\n",
            "[Epoch 39] Batch 500/511 KD-Loss=0.1853\n",
            "[Epoch 39] Val Loss=0.5956, Val Acc=0.6253  (epoch time 65.2s)\n",
            "[Epoch 40] Batch 100/511 KD-Loss=0.1851\n",
            "[Epoch 40] Batch 200/511 KD-Loss=0.1850\n",
            "[Epoch 40] Batch 300/511 KD-Loss=0.1853\n",
            "[Epoch 40] Batch 400/511 KD-Loss=0.1850\n",
            "[Epoch 40] Batch 500/511 KD-Loss=0.1851\n",
            "[Epoch 40] Val Loss=0.5954, Val Acc=0.6247  (epoch time 65.4s)\n",
            "[Epoch 41] Batch 100/511 KD-Loss=0.1850\n",
            "[Epoch 41] Batch 200/511 KD-Loss=0.1849\n",
            "[Epoch 41] Batch 300/511 KD-Loss=0.1847\n",
            "[Epoch 41] Batch 400/511 KD-Loss=0.1849\n",
            "[Epoch 41] Batch 500/511 KD-Loss=0.1849\n",
            "[Epoch 41] Val Loss=0.5952, Val Acc=0.6269  (epoch time 65.4s)\n",
            "[Epoch 42] Batch 100/511 KD-Loss=0.1847\n",
            "[Epoch 42] Batch 200/511 KD-Loss=0.1848\n",
            "[Epoch 42] Batch 300/511 KD-Loss=0.1847\n",
            "[Epoch 42] Batch 400/511 KD-Loss=0.1849\n",
            "[Epoch 42] Batch 500/511 KD-Loss=0.1845\n",
            "[Epoch 42] Val Loss=0.5950, Val Acc=0.6266  (epoch time 65.3s)\n",
            "[Epoch 43] Batch 100/511 KD-Loss=0.1845\n",
            "[Epoch 43] Batch 200/511 KD-Loss=0.1847\n",
            "[Epoch 43] Batch 300/511 KD-Loss=0.1847\n",
            "[Epoch 43] Batch 400/511 KD-Loss=0.1844\n",
            "[Epoch 43] Batch 500/511 KD-Loss=0.1846\n",
            "[Epoch 43] Val Loss=0.5944, Val Acc=0.6268  (epoch time 65.3s)\n",
            "[Epoch 44] Batch 100/511 KD-Loss=0.1845\n",
            "[Epoch 44] Batch 200/511 KD-Loss=0.1845\n",
            "[Epoch 44] Batch 300/511 KD-Loss=0.1845\n",
            "[Epoch 44] Batch 400/511 KD-Loss=0.1845\n",
            "[Epoch 44] Batch 500/511 KD-Loss=0.1844\n",
            "[Epoch 44] Val Loss=0.5951, Val Acc=0.6265  (epoch time 65.3s)\n",
            "[Epoch 45] Batch 100/511 KD-Loss=0.1846\n",
            "[Epoch 45] Batch 200/511 KD-Loss=0.1845\n",
            "[Epoch 45] Batch 300/511 KD-Loss=0.1842\n",
            "[Epoch 45] Batch 400/511 KD-Loss=0.1844\n",
            "[Epoch 45] Batch 500/511 KD-Loss=0.1843\n",
            "[Epoch 45] Val Loss=0.5943, Val Acc=0.6278  (epoch time 65.2s)\n",
            "[Epoch 46] Batch 100/511 KD-Loss=0.1843\n",
            "[Epoch 46] Batch 200/511 KD-Loss=0.1842\n",
            "[Epoch 46] Batch 300/511 KD-Loss=0.1843\n",
            "[Epoch 46] Batch 400/511 KD-Loss=0.1843\n",
            "[Epoch 46] Batch 500/511 KD-Loss=0.1845\n",
            "[Epoch 46] Val Loss=0.5939, Val Acc=0.6277  (epoch time 65.3s)\n",
            "[Epoch 47] Batch 100/511 KD-Loss=0.1842\n",
            "[Epoch 47] Batch 200/511 KD-Loss=0.1844\n",
            "[Epoch 47] Batch 300/511 KD-Loss=0.1842\n",
            "[Epoch 47] Batch 400/511 KD-Loss=0.1843\n",
            "[Epoch 47] Batch 500/511 KD-Loss=0.1842\n",
            "[Epoch 47] Val Loss=0.5941, Val Acc=0.6284  (epoch time 65.2s)\n",
            "[Epoch 48] Batch 100/511 KD-Loss=0.1843\n",
            "[Epoch 48] Batch 200/511 KD-Loss=0.1841\n",
            "[Epoch 48] Batch 300/511 KD-Loss=0.1841\n",
            "[Epoch 48] Batch 400/511 KD-Loss=0.1842\n",
            "[Epoch 48] Batch 500/511 KD-Loss=0.1843\n",
            "[Epoch 48] Val Loss=0.5942, Val Acc=0.6269  (epoch time 65.3s)\n",
            "[Epoch 49] Batch 100/511 KD-Loss=0.1841\n",
            "[Epoch 49] Batch 200/511 KD-Loss=0.1843\n",
            "[Epoch 49] Batch 300/511 KD-Loss=0.1843\n",
            "[Epoch 49] Batch 400/511 KD-Loss=0.1841\n",
            "[Epoch 49] Batch 500/511 KD-Loss=0.1842\n",
            "[Epoch 49] Val Loss=0.5942, Val Acc=0.6290  (epoch time 65.3s)\n",
            "[Epoch 50] Batch 100/511 KD-Loss=0.1844\n",
            "[Epoch 50] Batch 200/511 KD-Loss=0.1842\n",
            "[Epoch 50] Batch 300/511 KD-Loss=0.1840\n",
            "[Epoch 50] Batch 400/511 KD-Loss=0.1841\n",
            "[Epoch 50] Batch 500/511 KD-Loss=0.1841\n",
            "[Epoch 50] Val Loss=0.5941, Val Acc=0.6292  (epoch time 66.6s)\n",
            "[POST-KD] Val Loss=0.5941, Val Acc=0.6292\n",
            "[INFO] KD student saved -> /content/drive/MyDrive/AUV Implenation/student_kd.pth\n",
            "[INFO] Metrics saved -> /content/drive/MyDrive/AUV Implenation/student_kd_metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Test Saved KD Student Model\n",
        "# =========================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -------------------------------\n",
        "# Paths\n",
        "# -------------------------------\n",
        "DATA_PATH = \"/content/drive/MyDrive/AUV Implenation/balanced_sample_200MB.csv\"  # dataset to test\n",
        "STUDENT_SAVE_PATH = \"/content/drive/MyDrive/AUV Implenation/student_kd.pth\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 8192\n",
        "\n",
        "print(f\"[INFO] Device: {DEVICE}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Student Architecture (must match training)\n",
        "# -------------------------------\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# -------------------------------\n",
        "# Safe load for PyTorch 2.6+ (weights + metadata)\n",
        "# -------------------------------\n",
        "import torch\n",
        "torch.serialization.add_safe_globals([__import__('numpy')._core.multiarray._reconstruct])\n",
        "\n",
        "ckpt = torch.load(STUDENT_SAVE_PATH, map_location=DEVICE, weights_only=False)\n",
        "input_dim = ckpt[\"input_dim\"]\n",
        "num_classes = ckpt[\"num_classes\"]\n",
        "mean_ = ckpt[\"standardize_mean\"]\n",
        "std_  = ckpt[\"standardize_std\"]\n",
        "\n",
        "# -------------------------------\n",
        "# Build student model and load weights\n",
        "# -------------------------------\n",
        "student_hidden = 32\n",
        "student = StudentNet(input_dim=input_dim, hidden_dim=student_hidden, num_classes=num_classes).to(DEVICE)\n",
        "student.load_state_dict(ckpt[\"state_dict\"])\n",
        "student.eval()\n",
        "print(f\"[INFO] Student model loaded from {STUDENT_SAVE_PATH}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Load dataset & standardize\n",
        "# -------------------------------\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
        "\n",
        "X = df.iloc[:, :-1].values.astype(np.float32)\n",
        "y = df.iloc[:, -1].values.astype(np.int64)\n",
        "\n",
        "# Standardize using training mean/std from KD\n",
        "X_std = (X - mean_) / std_\n",
        "\n",
        "X_tensor = torch.tensor(X_std, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    TensorDataset(X_tensor, y_tensor),\n",
        "    batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluation helper\n",
        "# -------------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for xb, yb in data_loader:\n",
        "        xb = xb.to(DEVICE, non_blocking=True)\n",
        "        yb = yb.to(DEVICE, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb, reduction=\"sum\")\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "        loss_sum += loss.item()\n",
        "    acc = correct / max(1, total)\n",
        "    return loss_sum / max(1, total), acc\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate\n",
        "# -------------------------------\n",
        "val_loss, val_acc = evaluate(student, data_loader)\n",
        "print(f\"[TEST] Dataset Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D86O-7ekMzx0",
        "outputId": "4be4e87a-e93b-43ff-9e42-2aafc100c246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Device: cuda\n",
            "[INFO] Student model loaded from /content/drive/MyDrive/AUV Implenation/student_kd.pth\n",
            "[TEST] Dataset Val Loss=0.5945, Val Acc=0.6290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Test Saved Student Model (Old or KD)\n",
        "# =========================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# -------------------------------\n",
        "# Paths\n",
        "# -------------------------------\n",
        "DATA_PATH = \"/content/drive/MyDrive/AUV Implenation/balanced_sample_200MB.csv\"\n",
        "STUDENT_SAVE_PATH = \"/content/drive/MyDrive/AUV Implenation/final_model_gpu.pt\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 8192\n",
        "print(f\"[INFO] Device: {DEVICE}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Student Architecture\n",
        "# -------------------------------\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# -------------------------------\n",
        "# Load checkpoint safely\n",
        "# -------------------------------\n",
        "import torch\n",
        "torch.serialization.add_safe_globals([__import__('numpy')._core.multiarray._reconstruct])\n",
        "\n",
        "ckpt = torch.load(STUDENT_SAVE_PATH, map_location=DEVICE, weights_only=False)\n",
        "\n",
        "# Determine how to extract state_dict\n",
        "if isinstance(ckpt, dict):\n",
        "    if \"state_dict\" in ckpt:\n",
        "        state_dict = ckpt[\"state_dict\"]\n",
        "        input_dim = ckpt.get(\"input_dim\", 4)\n",
        "        num_classes = ckpt.get(\"num_classes\", 2)\n",
        "        mean_ = ckpt.get(\"standardize_mean\", np.zeros((1, input_dim)))\n",
        "        std_ = ckpt.get(\"standardize_std\", np.ones((1, input_dim)))\n",
        "    elif \"model_state_dict\" in ckpt:  # old-style checkpoint\n",
        "        state_dict = ckpt[\"model_state_dict\"]\n",
        "        input_dim = ckpt.get(\"features\", 4)  # fallback if not in ckpt\n",
        "        num_classes = ckpt.get(\"classes\", 2)\n",
        "        mean_ = np.zeros((1, input_dim))\n",
        "        std_ = np.ones((1, input_dim))\n",
        "    else:\n",
        "        # assume dict is just weights\n",
        "        state_dict = ckpt\n",
        "        input_dim = 4\n",
        "        num_classes = 2\n",
        "        mean_ = np.zeros((1, input_dim))\n",
        "        std_ = np.ones((1, input_dim))\n",
        "else:\n",
        "    # weights-only tensor\n",
        "    state_dict = ckpt\n",
        "    input_dim = 4\n",
        "    num_classes = 2\n",
        "    mean_ = np.zeros((1, input_dim))\n",
        "    std_ = np.ones((1, input_dim))\n",
        "\n",
        "# -------------------------------\n",
        "# Build model and load weights\n",
        "# -------------------------------\n",
        "student = StudentNet(input_dim=input_dim, hidden_dim=32, num_classes=num_classes).to(DEVICE)\n",
        "\n",
        "# Filter state_dict to match StudentNet keys\n",
        "filtered_dict = {k.replace(\"net.\", \"\"): v for k, v in state_dict.items() if \"net\" in k or k in student.state_dict()}\n",
        "student.load_state_dict(filtered_dict, strict=False)\n",
        "student.eval()\n",
        "print(f\"[INFO] Student model loaded from {STUDENT_SAVE_PATH}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Load dataset & standardize\n",
        "# -------------------------------\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
        "\n",
        "X = df.iloc[:, :-1].values.astype(np.float32)\n",
        "y = df.iloc[:, -1].values.astype(np.int64)\n",
        "X_std = (X - mean_) / std_\n",
        "\n",
        "X_tensor = torch.tensor(X_std, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    TensorDataset(X_tensor, y_tensor),\n",
        "    batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluation helper\n",
        "# -------------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for xb, yb in data_loader:\n",
        "        xb = xb.to(DEVICE, non_blocking=True)\n",
        "        yb = yb.to(DEVICE, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb, reduction=\"sum\")\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "        loss_sum += loss.item()\n",
        "    acc = correct / max(1, total)\n",
        "    return loss_sum / max(1, total), acc\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate\n",
        "# -------------------------------\n",
        "val_loss, val_acc = evaluate(student, data_loader)\n",
        "print(f\"[TEST] Dataset Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wr5RZxKNzgR",
        "outputId": "41878580-e32e-4d91-a3ec-0c4a3cde1a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Device: cuda\n",
            "[INFO] Student model loaded from /content/drive/MyDrive/AUV Implenation/final_model_gpu.pt\n",
            "[TEST] Dataset Val Loss=70.3039, Val Acc=0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/AUV Implenation/balanced_sample_200MB.csv\")\n",
        "print(df.iloc[:, -1].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZJ3Fb--pkAW",
        "outputId": "6d914bf9-f9b3-4c02-fbf9-4ad7cc073495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "0    2612817\n",
            "1    2612817\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Federated Learning + Simple Blockchain ledger demo (no Ray/Flower)\n",
        "\n",
        "Fixed: robust student checkpoint loader, AMP usage, label checks, safer saving.\n",
        "All errors resolved including Subset evaluation bug and deprecated warnings.\n",
        "\"\"\"\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import hashlib\n",
        "import math\n",
        "from copy import deepcopy\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_auc_score, matthews_corrcoef\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For safe checkpoint fallback\n",
        "from torch.serialization import add_safe_globals\n",
        "import zipfile\n",
        "\n",
        "# ---------------------------\n",
        "# USER CONFIGURE THESE PATHS\n",
        "# ---------------------------\n",
        "CSV_PATH = \"/content/drive/MyDrive/AUV Implenation/balanced_sample_200MB.csv\"  # <-- change to your CSV path\n",
        "STUDENT_MODEL_PATH = \"/content/drive/MyDrive/MyDrive/AUV Implenation/student_kd.pth\"  # or None\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/AUV Implenation/fl_blockchain_results\"\n",
        "\n",
        "# ---------------------------\n",
        "# TRAINING / FL CONFIG\n",
        "# ---------------------------\n",
        "NUM_CLIENTS_REQUESTED = 3\n",
        "BATCH_SIZE = 1024\n",
        "NUM_WORKERS = 0\n",
        "LOCAL_EPOCHS = 1\n",
        "ROUNDS = 50\n",
        "LR = 3e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "SEED = 42\n",
        "VALIDATION_SPLIT = 0.1\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", DEVICE)\n",
        "if DEVICE.type == \"cuda\":\n",
        "    try:\n",
        "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# MODEL DEFINITION\n",
        "# ---------------------------\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, inp, out):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(inp, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ---------------------------\n",
        "# Blockchain simple implementation\n",
        "# ---------------------------\n",
        "class Block:\n",
        "    def __init__(self, index: int, timestamp: float, update_hash: str, prev_hash: str, meta: Dict[str, Any]):\n",
        "        self.index = index\n",
        "        self.timestamp = timestamp\n",
        "        self.update_hash = update_hash\n",
        "        self.prev_hash = prev_hash\n",
        "        self.meta = meta\n",
        "        self.hash = self.compute_hash()\n",
        "\n",
        "    def compute_hash(self) -> str:\n",
        "        payload = f\"{self.index}|{self.timestamp}|{self.update_hash}|{self.prev_hash}|{json.dumps(self.meta, sort_keys=True)}\"\n",
        "        return hashlib.sha256(payload.encode()).hexdigest()\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"index\": self.index,\n",
        "            \"timestamp\": self.timestamp,\n",
        "            \"update_hash\": self.update_hash,\n",
        "            \"prev_hash\": self.prev_hash,\n",
        "            \"meta\": self.meta,\n",
        "            \"hash\": self.hash\n",
        "        }\n",
        "\n",
        "class SimpleBlockchain:\n",
        "    def __init__(self):\n",
        "        self.chain: List[Block] = []\n",
        "        genesis = Block(0, time.time(), update_hash=\"0\"*64, prev_hash=\"0\"*64, meta={\"note\": \"genesis\"})\n",
        "        self.chain.append(genesis)\n",
        "\n",
        "    def add_block(self, update_bytes: bytes, meta: Dict[str, Any]) -> Block:\n",
        "        update_hash = hashlib.sha256(update_bytes).hexdigest()\n",
        "        prev_hash = self.chain[-1].hash\n",
        "        block = Block(len(self.chain), time.time(), update_hash, prev_hash, meta)\n",
        "        if self.verify_block(block, update_bytes):\n",
        "            self.chain.append(block)\n",
        "            return block\n",
        "        else:\n",
        "            raise ValueError(\"Block verification failed\")\n",
        "\n",
        "    def verify_block(self, block: Block, update_bytes: bytes) -> bool:\n",
        "        recomputed = hashlib.sha256(update_bytes).hexdigest()\n",
        "        ok = (recomputed == block.update_hash) and (block.prev_hash == self.chain[-1].hash)\n",
        "        return ok\n",
        "\n",
        "    def to_json(self):\n",
        "        return [blk.to_dict() for blk in self.chain]\n",
        "\n",
        "# ---------------------------\n",
        "# Robust student checkpoint loader\n",
        "# ---------------------------\n",
        "def try_load_student_checkpoint(path, map_location=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Attempt to safely load a student checkpoint.\n",
        "    Returns loaded object or None on failure.\n",
        "    \"\"\"\n",
        "    if path is None or not os.path.isfile(path):\n",
        "        if verbose:\n",
        "            print(f\"[LOAD] Student checkpoint not found at: {path}\")\n",
        "        return None\n",
        "\n",
        "    # 1) Try weights_only=True (safe): this expects a state_dict mapping name->tensor\n",
        "    try:\n",
        "        obj = torch.load(path, map_location=map_location, weights_only=True)\n",
        "        if verbose:\n",
        "            print(\"[LOAD] Loaded checkpoint with weights_only=True.\")\n",
        "        return obj\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"[LOAD] weights_only=True failed: {type(e).__name__}: {e}\")\n",
        "\n",
        "    # 2) Try weights_only=False but with safe allowlist for numpy reconstruct (trusted)\n",
        "    try:\n",
        "        # attempt to find numpy reconstruct function\n",
        "        safe_fn = None\n",
        "        try:\n",
        "            # In many numpy builds it's np.core.multiarray._reconstruct\n",
        "            safe_fn = getattr(getattr(np, \"core\"), \"multiarray\")._reconstruct\n",
        "        except Exception:\n",
        "            safe_fn = None\n",
        "\n",
        "        if safe_fn is not None:\n",
        "            if verbose:\n",
        "                print(\"[LOAD] Attempting torch.load(..., weights_only=False) with safe_globals for numpy reconstruct.\")\n",
        "            with add_safe_globals([safe_fn]):\n",
        "                obj = torch.load(path, map_location=map_location, weights_only=False)\n",
        "            if verbose:\n",
        "                print(\"[LOAD] Loaded checkpoint with safe_globals (weights_only=False).\")\n",
        "            return obj\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"[LOAD] Could not locate numpy reconstruct function; skipping safe-globals fallback.\")\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"[LOAD] weights_only=False with safe_globals failed: {type(e).__name__}: {e}\")\n",
        "\n",
        "    # 3) If zip archive, show contents (helpful for debugging)\n",
        "    try:\n",
        "        with zipfile.ZipFile(path, \"r\") as z:\n",
        "            if verbose:\n",
        "                print(\"[LOAD] Checkpoint appears to be archive. Contents:\")\n",
        "                for info in z.infolist():\n",
        "                    print(\"   \", info.filename)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if verbose:\n",
        "        print(\"[LOAD] Failed to load checkpoint safely. Will continue with random init.\")\n",
        "    return None\n",
        "\n",
        "# ---------------------------\n",
        "# Utility helpers\n",
        "# ---------------------------\n",
        "def load_and_preprocess(csv_path):\n",
        "    print(f\"[INFO] Loading CSV: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # Use the integer labels directly\n",
        "    y = df.iloc[:, -1].values\n",
        "    X_df = df.iloc[:, :-1]\n",
        "\n",
        "    categorical_cols = X_df.select_dtypes(include='object').columns.tolist()\n",
        "    numeric_cols = X_df.select_dtypes(exclude='object').columns.tolist()\n",
        "\n",
        "    if len(categorical_cols) > 0:\n",
        "        ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "        X_cat = ohe.fit_transform(X_df[categorical_cols])\n",
        "    else:\n",
        "        X_cat = np.empty((len(X_df), 0))\n",
        "\n",
        "    if len(numeric_cols) > 0:\n",
        "        scaler = StandardScaler()\n",
        "        X_num = scaler.fit_transform(X_df[numeric_cols])\n",
        "    else:\n",
        "        X_num = np.empty((len(X_df), 0))\n",
        "\n",
        "    X = np.hstack([X_num, X_cat]).astype(np.float32)\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    print(f\"[INFO] Data: samples={X.shape[0]} | features={X.shape[1]} | class_counts={dict(zip(unique.tolist(), counts.tolist()))}\")\n",
        "    if len(unique) < 2:\n",
        "        raise RuntimeError(\"Dataset contains only a single class after mapping. Fix labels or CSV.\")\n",
        "    return X, y.astype(int)\n",
        "\n",
        "def model_state_to_vector(state_dict: Dict[str, torch.Tensor]) -> np.ndarray:\n",
        "    parts = []\n",
        "    for k, v in state_dict.items():\n",
        "        arr = v.detach().cpu().numpy().ravel()\n",
        "        parts.append(arr)\n",
        "    if len(parts) == 0:\n",
        "        return np.array([], dtype=np.float32)\n",
        "    return np.concatenate(parts).astype(np.float32)\n",
        "\n",
        "def vector_to_state_dict_example(vector: np.ndarray, template_state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    new_state = {}\n",
        "    pos = 0\n",
        "    for k, v in template_state.items():\n",
        "        size = v.numel()\n",
        "        chunk = vector[pos:pos+size].reshape(v.shape)\n",
        "        new_state[k] = torch.tensor(chunk, dtype=v.dtype)\n",
        "        pos += size\n",
        "    return new_state\n",
        "\n",
        "def subtract_state_dicts(a: Dict[str, torch.Tensor], b: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    out = {}\n",
        "    for k in a.keys():\n",
        "        out[k] = a[k].detach().cpu() - b[k].detach().cpu()\n",
        "    return out\n",
        "\n",
        "def add_state_dicts_weighted(base: Dict[str, torch.Tensor], updates: List[Dict[str, torch.Tensor]], weights: List[float]) -> Dict[str, torch.Tensor]:\n",
        "    res = {}\n",
        "    for k in base.keys():\n",
        "        tensor = base[k].detach().cpu().clone()\n",
        "        if tensor.dtype == torch.float32 or tensor.dtype == torch.float64:\n",
        "            for upd, w in zip(updates, weights):\n",
        "                tensor += (upd[k].detach().cpu() * w)\n",
        "        else:\n",
        "            # For non-float tensors (like BatchNorm running stats), just average\n",
        "            for upd, w in zip(updates, weights):\n",
        "                 tensor += upd[k].detach().cpu()\n",
        "            tensor = tensor / len(updates) # Simple average\n",
        "\n",
        "        res[k] = tensor\n",
        "    return res\n",
        "\n",
        "def state_dict_to_numpy_bytes(sd: Dict[str, torch.Tensor]) -> bytes:\n",
        "    vec = model_state_to_vector(sd)\n",
        "    return vec.tobytes()\n",
        "\n",
        "# ---------------------------\n",
        "# Local client training (sequential simulation)\n",
        "# ---------------------------\n",
        "def client_local_train(global_state: Dict[str, torch.Tensor],\n",
        "                       dataset: TensorDataset,\n",
        "                       indices: List[int],\n",
        "                       client_id: int,\n",
        "                       local_epochs: int = 1,\n",
        "                       batch_size: int = 1024) -> Dict[str, torch.Tensor]:\n",
        "    features = dataset.tensors[0].shape[1]\n",
        "    classes = int(dataset.tensors[1].max().item()) + 1\n",
        "    model = Net(features, classes).to(DEVICE)\n",
        "    model.load_state_dict({k: v.to(DEVICE) for k, v in global_state.items()})\n",
        "\n",
        "    sub = Subset(dataset, indices)\n",
        "    loader = DataLoader(sub, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    # Fixed: Use updated torch.amp API\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "    model.train()\n",
        "    for ep in range(local_epochs):\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            # Fixed: Use updated torch.amp API\n",
        "            with torch.amp.autocast('cuda', enabled=(DEVICE.type == \"cuda\")):\n",
        "                logits = model(xb)\n",
        "                loss = loss_fn(logits, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "\n",
        "    new_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
        "    return new_state\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation helper - FIXED\n",
        "# ---------------------------\n",
        "def evaluate_state(state: Dict[str, torch.Tensor], dataset, batch_size: int = 2048):\n",
        "    \"\"\"\n",
        "    Fixed evaluation function that properly handles both TensorDataset and Subset objects.\n",
        "    \"\"\"\n",
        "    # Extract data from dataset - handle both TensorDataset and Subset\n",
        "    if isinstance(dataset, TensorDataset):\n",
        "        X_eval = dataset.tensors[0]\n",
        "        y_eval = dataset.tensors[1]\n",
        "    elif isinstance(dataset, Subset):\n",
        "        # Fixed: Properly extract tensors from Subset\n",
        "        indices = dataset.indices\n",
        "        original_dataset = dataset.dataset  # This should be TensorDataset\n",
        "        X_eval = original_dataset.tensors[0][indices]\n",
        "        y_eval = original_dataset.tensors[1][indices]\n",
        "    else:\n",
        "        raise TypeError(f\"Unsupported dataset type for evaluation: {type(dataset)}\")\n",
        "\n",
        "    # Handle empty dataset case\n",
        "    if y_eval.numel() == 0:\n",
        "        print(\"[WARN] Evaluation dataset is empty.\")\n",
        "        return {\n",
        "            'accuracy': 0.0, 'precision_macro': 0.0, 'recall_macro': 0.0,\n",
        "            'f1_macro': 0.0, 'mcc': None, 'roc_auc': None,\n",
        "            'confusion_matrix': [[0, 0], [0, 0]]\n",
        "        }\n",
        "\n",
        "    features = X_eval.shape[1]\n",
        "    classes = int(y_eval.max().item()) + 1\n",
        "\n",
        "    model = Net(features, classes).to(DEVICE)\n",
        "    model.load_state_dict({k: v.to(DEVICE) for k, v in state.items()})\n",
        "    model.eval()\n",
        "\n",
        "    loader = DataLoader(TensorDataset(X_eval, y_eval), batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
        "    y_true, y_pred, y_scores = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            logits = model(xb)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            preds = probs.argmax(dim=1)\n",
        "            y_true.extend(yb.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "            y_scores.extend(probs.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_scores = np.array(y_scores)\n",
        "\n",
        "    metrics = {}\n",
        "    metrics['accuracy'] = float(accuracy_score(y_true, y_pred))\n",
        "    metrics['precision_macro'] = float(precision_score(y_true, y_pred, average='macro', zero_division=0))\n",
        "    metrics['recall_macro'] = float(recall_score(y_true, y_pred, average='macro', zero_division=0))\n",
        "    metrics['f1_macro'] = float(f1_score(y_true, y_pred, average='macro', zero_division=0))\n",
        "    metrics['mcc'] = float(matthews_corrcoef(y_true, y_pred)) if len(np.unique(y_true))>1 else None\n",
        "\n",
        "    try:\n",
        "        metrics['roc_auc'] = float(roc_auc_score(y_true, y_scores[:,1])) if y_scores.shape[1] > 1 else None\n",
        "    except Exception:\n",
        "        metrics['roc_auc'] = None\n",
        "\n",
        "    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred).tolist()\n",
        "    return metrics\n",
        "\n",
        "# ---------------------------\n",
        "# Main federated loop\n",
        "# ---------------------------\n",
        "def run_federated_blockchain(X, y):\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    if DEVICE.type == \"cuda\":\n",
        "        torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    X_t = torch.tensor(X, dtype=torch.float32)\n",
        "    y_t = torch.tensor(y, dtype=torch.long)\n",
        "    dataset = TensorDataset(X_t, y_t)\n",
        "\n",
        "    idxs = np.arange(len(dataset))\n",
        "    train_idxs, val_idxs = train_test_split(idxs, test_size=VALIDATION_SPLIT, random_state=SEED, stratify=y)\n",
        "    val_dataset = Subset(dataset, val_idxs)\n",
        "\n",
        "    requested = max(1, NUM_CLIENTS_REQUESTED)\n",
        "    client_splits = np.array_split(train_idxs, requested)\n",
        "    client_splits = [arr.tolist() for arr in client_splits if len(arr)>0]\n",
        "    NUM_CLIENTS = len(client_splits)\n",
        "    print(f\"[INFO] NUM_CLIENTS actual: {NUM_CLIENTS}\")\n",
        "\n",
        "    features = X.shape[1]\n",
        "    classes = int(y.max()) + 1\n",
        "    global_model = Net(features, classes).to(DEVICE)\n",
        "\n",
        "    # Try to load student model robustly\n",
        "    if STUDENT_MODEL_PATH is not None and os.path.isfile(STUDENT_MODEL_PATH):\n",
        "        print(\"[INFO] Attempting to load student model from:\", STUDENT_MODEL_PATH)\n",
        "        loaded = try_load_student_checkpoint(STUDENT_MODEL_PATH, map_location=DEVICE, verbose=True)\n",
        "        if loaded is None:\n",
        "            print(\"[WARN] Student model could not be loaded safely; using random initialization.\")\n",
        "        else:\n",
        "            # try to detect format\n",
        "            try:\n",
        "                if isinstance(loaded, dict) and all(isinstance(v, torch.Tensor) for v in loaded.values()):\n",
        "                    global_model.load_state_dict(loaded)\n",
        "                    print(\"[INFO] Loaded student state_dict into model.\")\n",
        "                elif isinstance(loaded, dict) and \"model_state\" in loaded:\n",
        "                    global_model.load_state_dict(loaded[\"model_state\"])\n",
        "                    print(\"[INFO] Loaded checkpoint['model_state'] into model.\")\n",
        "                elif isinstance(loaded, dict) and \"state_dict\" in loaded:\n",
        "                    global_model.load_state_dict(loaded[\"state_dict\"])\n",
        "                    print(\"[INFO] Loaded checkpoint['state_dict'] into model.\")\n",
        "                else:\n",
        "                    # try to see if it's a mapping of numpy arrays -> convert\n",
        "                    sd_try = {}\n",
        "                    for k, v in loaded.items():\n",
        "                        try:\n",
        "                            sd_try[k] = torch.tensor(v)\n",
        "                        except Exception:\n",
        "                            sd_try = None\n",
        "                            break\n",
        "                    if sd_try is not None:\n",
        "                        global_model.load_state_dict(sd_try)\n",
        "                        print(\"[INFO] Loaded converted numpy->tensor state into model.\")\n",
        "                    else:\n",
        "                        print(\"[WARN] Loaded checkpoint format not recognized. Starting from random init.\")\n",
        "            except Exception as e:\n",
        "                print(\"[WARN] Failed to load checkpoint into model:\", e)\n",
        "                print(\"Continuing with random init.\")\n",
        "\n",
        "    global_state = {k: v.detach().cpu().clone() for k, v in global_model.state_dict().items()}\n",
        "\n",
        "    ledger = SimpleBlockchain()\n",
        "    ledger_debug_log = []\n",
        "    rounds_history = []\n",
        "\n",
        "    for rnd in range(1, ROUNDS+1):\n",
        "        print(f\"\\n--- ROUND {rnd:03d} ---\")\n",
        "        start_round = time.time()\n",
        "        client_updates = []\n",
        "        client_num_samples = []\n",
        "\n",
        "        for cid, c_idxs in enumerate(client_splits):\n",
        "            print(f\"[Client {cid}] Training on {len(c_idxs)} samples (round {rnd})\")\n",
        "            new_state = client_local_train(global_state, dataset, c_idxs, client_id=cid, local_epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE)\n",
        "            delta = subtract_state_dicts(new_state, global_state)\n",
        "            update_bytes = state_dict_to_numpy_bytes(delta)\n",
        "            meta = {\"client_id\": cid, \"round\": rnd, \"num_samples\": len(c_idxs)}\n",
        "            try:\n",
        "                block = ledger.add_block(update_bytes, meta)\n",
        "                ledger_debug_log.append({\"action\":\"add_block\", \"round\": rnd, \"client\": cid, \"block_index\": block.index, \"block_hash\": block.hash})\n",
        "                print(f\"[Blockchain] Block added for client {cid} -> block_index={block.index} hash={block.hash[:12]}...\")\n",
        "                client_updates.append(delta)\n",
        "                client_num_samples.append(len(c_idxs))\n",
        "            except Exception as e:\n",
        "                print(f\"[Blockchain] Block verification failed for client {cid}: {e}\")\n",
        "                ledger_debug_log.append({\"action\":\"failed_block\", \"client\": cid, \"err\": str(e)})\n",
        "\n",
        "        if len(client_updates) == 0:\n",
        "            print(\"No verified updates this round. Skipping aggregation.\")\n",
        "            continue\n",
        "\n",
        "        total_samples = sum(client_num_samples)\n",
        "        weights = [n / total_samples for n in client_num_samples]\n",
        "        new_global_state = add_state_dicts_weighted(global_state, client_updates, weights)\n",
        "        global_state = new_global_state\n",
        "\n",
        "        round_time = time.time() - start_round\n",
        "        val_metrics = evaluate_state(global_state, val_dataset, batch_size=BATCH_SIZE)\n",
        "        print(f\"[Round {rnd}] val_acc={val_metrics['accuracy']:.4f} val_f1={val_metrics['f1_macro']:.4f} time={round_time:.2f}s\")\n",
        "\n",
        "        rounds_history.append({\n",
        "            \"round\": rnd,\n",
        "            \"val_accuracy\": val_metrics['accuracy'],\n",
        "            \"val_f1_macro\": val_metrics['f1_macro'],\n",
        "            \"val_precision_macro\": val_metrics['precision_macro'],\n",
        "            \"val_recall_macro\": val_metrics['recall_macro'],\n",
        "            \"val_roc_auc\": val_metrics['roc_auc'],\n",
        "            \"num_verified_updates\": len(client_updates),\n",
        "            \"round_time_s\": round_time\n",
        "        })\n",
        "\n",
        "        if rnd % 5 == 0 or rnd == ROUNDS:\n",
        "            torch.save(global_state, os.path.join(OUTPUT_DIR, f\"global_state_round{rnd}.pth\"))\n",
        "            with open(os.path.join(OUTPUT_DIR, \"ledger.json\"), \"w\") as f:\n",
        "                json.dump(ledger.to_json(), f, indent=2)\n",
        "\n",
        "    final_model_path = os.path.join(OUTPUT_DIR, \"final_model.pth\")\n",
        "    torch.save(global_state, final_model_path)\n",
        "    print(\"[INFO] Final global model saved to\", final_model_path)\n",
        "\n",
        "    with open(os.path.join(OUTPUT_DIR, \"ledger.json\"), \"w\") as f:\n",
        "        json.dump(ledger.to_json(), f, indent=2)\n",
        "    with open(os.path.join(OUTPUT_DIR, \"ledger_debug.json\"), \"w\") as f:\n",
        "        json.dump(ledger_debug_log, f, indent=2)\n",
        "\n",
        "    df_hist = pd.DataFrame(rounds_history)\n",
        "    metrics_csv = os.path.join(OUTPUT_DIR, \"federated_metrics_rounds.csv\")\n",
        "    df_hist.to_csv(metrics_csv, index=False)\n",
        "    print(\"[INFO] Metrics saved to:\", metrics_csv)\n",
        "\n",
        "    # plots\n",
        "    try:\n",
        "        plt.figure()\n",
        "        plt.plot(df_hist[\"round\"], df_hist[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "        plt.xlabel(\"Round\"); plt.ylabel(\"Validation accuracy\"); plt.title(\"Validation Accuracy per Round\")\n",
        "        plt.grid(True); plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, \"val_accuracy.png\")); plt.close()\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(df_hist[\"round\"], df_hist[\"val_f1_macro\"], label=\"val_f1_macro\")\n",
        "        plt.xlabel(\"Round\"); plt.ylabel(\"Validation F1 (macro)\"); plt.title(\"Validation F1 per Round\")\n",
        "        plt.grid(True); plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, \"val_f1.png\")); plt.close()\n",
        "        print(\"[INFO] Plots saved to\", OUTPUT_DIR)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Plotting failed:\", e)\n",
        "\n",
        "    return final_model_path, metrics_csv, os.path.join(OUTPUT_DIR, \"ledger.json\")\n",
        "\n",
        "# ---------------------------\n",
        "# Run\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    X, y = load_and_preprocess(CSV_PATH)\n",
        "    final_model_path, metrics_csv, ledger_path = run_federated_blockchain(X, y)\n",
        "    tot = time.time() - start\n",
        "    print(f\"\\nALL DONE in {tot/60:.2f} minutes. Final model: {final_model_path}, metrics: {metrics_csv}, ledger: {ledger_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S60DANV9oLvV",
        "outputId": "8da0d85c-28bb-495b-c0a3-5e7c6e7e63aa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA L4\n",
            "[INFO] Loading CSV: /content/drive/MyDrive/AUV Implenation/balanced_sample_200MB.csv\n",
            "[INFO] Data: samples=5225634 | features=4 | class_counts={0: 2612817, 1: 2612817}\n",
            "[INFO] NUM_CLIENTS actual: 3\n",
            "\n",
            "--- ROUND 001 ---\n",
            "[Client 0] Training on 1567690 samples (round 1)\n",
            "[Blockchain] Block added for client 0 -> block_index=1 hash=fee640dbc2ca...\n",
            "[Client 1] Training on 1567690 samples (round 1)\n",
            "[Blockchain] Block added for client 1 -> block_index=2 hash=a7390dbe5c0b...\n",
            "[Client 2] Training on 1567690 samples (round 1)\n",
            "[Blockchain] Block added for client 2 -> block_index=3 hash=4db18e9dec39...\n",
            "[Round 1] val_acc=0.6147 val_f1=0.5749 time=61.86s\n",
            "\n",
            "--- ROUND 002 ---\n",
            "[Client 0] Training on 1567690 samples (round 2)\n",
            "[Blockchain] Block added for client 0 -> block_index=4 hash=ba32aaa16323...\n",
            "[Client 1] Training on 1567690 samples (round 2)\n",
            "[Blockchain] Block added for client 1 -> block_index=5 hash=b8995d36695f...\n",
            "[Client 2] Training on 1567690 samples (round 2)\n",
            "[Blockchain] Block added for client 2 -> block_index=6 hash=34357eeb5ea3...\n",
            "[Round 2] val_acc=0.6353 val_f1=0.6115 time=55.40s\n",
            "\n",
            "--- ROUND 003 ---\n",
            "[Client 0] Training on 1567690 samples (round 3)\n",
            "[Blockchain] Block added for client 0 -> block_index=7 hash=a6ec8d47f35b...\n",
            "[Client 1] Training on 1567690 samples (round 3)\n",
            "[Blockchain] Block added for client 1 -> block_index=8 hash=21f0dfa41075...\n",
            "[Client 2] Training on 1567690 samples (round 3)\n",
            "[Blockchain] Block added for client 2 -> block_index=9 hash=e84c003f22d3...\n",
            "[Round 3] val_acc=0.6380 val_f1=0.6145 time=55.38s\n",
            "\n",
            "--- ROUND 004 ---\n",
            "[Client 0] Training on 1567690 samples (round 4)\n",
            "[Blockchain] Block added for client 0 -> block_index=10 hash=f18eabfc2460...\n",
            "[Client 1] Training on 1567690 samples (round 4)\n",
            "[Blockchain] Block added for client 1 -> block_index=11 hash=dc7d28e0f428...\n",
            "[Client 2] Training on 1567690 samples (round 4)\n",
            "[Blockchain] Block added for client 2 -> block_index=12 hash=bf5680d07190...\n",
            "[Round 4] val_acc=0.6407 val_f1=0.6158 time=55.03s\n",
            "\n",
            "--- ROUND 005 ---\n",
            "[Client 0] Training on 1567690 samples (round 5)\n",
            "[Blockchain] Block added for client 0 -> block_index=13 hash=5d60ecc7d072...\n",
            "[Client 1] Training on 1567690 samples (round 5)\n",
            "[Blockchain] Block added for client 1 -> block_index=14 hash=e29d7627b4ac...\n",
            "[Client 2] Training on 1567690 samples (round 5)\n",
            "[Blockchain] Block added for client 2 -> block_index=15 hash=4eb25708a40e...\n",
            "[Round 5] val_acc=0.6408 val_f1=0.6182 time=54.61s\n",
            "\n",
            "--- ROUND 006 ---\n",
            "[Client 0] Training on 1567690 samples (round 6)\n",
            "[Blockchain] Block added for client 0 -> block_index=16 hash=ba1d62e69438...\n",
            "[Client 1] Training on 1567690 samples (round 6)\n",
            "[Blockchain] Block added for client 1 -> block_index=17 hash=bff40ba8e0ab...\n",
            "[Client 2] Training on 1567690 samples (round 6)\n",
            "[Blockchain] Block added for client 2 -> block_index=18 hash=481dd122638a...\n",
            "[Round 6] val_acc=0.6412 val_f1=0.6183 time=54.97s\n",
            "\n",
            "--- ROUND 007 ---\n",
            "[Client 0] Training on 1567690 samples (round 7)\n",
            "[Blockchain] Block added for client 0 -> block_index=19 hash=d5f24e5a5933...\n",
            "[Client 1] Training on 1567690 samples (round 7)\n",
            "[Blockchain] Block added for client 1 -> block_index=20 hash=c0c011d379dc...\n",
            "[Client 2] Training on 1567690 samples (round 7)\n",
            "[Blockchain] Block added for client 2 -> block_index=21 hash=143c58ef8e7a...\n",
            "[Round 7] val_acc=0.6430 val_f1=0.6247 time=55.35s\n",
            "\n",
            "--- ROUND 008 ---\n",
            "[Client 0] Training on 1567690 samples (round 8)\n",
            "[Blockchain] Block added for client 0 -> block_index=22 hash=60f8be386d2c...\n",
            "[Client 1] Training on 1567690 samples (round 8)\n",
            "[Blockchain] Block added for client 1 -> block_index=23 hash=96cac0219188...\n",
            "[Client 2] Training on 1567690 samples (round 8)\n",
            "[Blockchain] Block added for client 2 -> block_index=24 hash=693c4032d2a4...\n",
            "[Round 8] val_acc=0.6451 val_f1=0.6240 time=55.19s\n",
            "\n",
            "--- ROUND 009 ---\n",
            "[Client 0] Training on 1567690 samples (round 9)\n",
            "[Blockchain] Block added for client 0 -> block_index=25 hash=ecd4ba8d0028...\n",
            "[Client 1] Training on 1567690 samples (round 9)\n",
            "[Blockchain] Block added for client 1 -> block_index=26 hash=61f508efa48c...\n",
            "[Client 2] Training on 1567690 samples (round 9)\n",
            "[Blockchain] Block added for client 2 -> block_index=27 hash=cad1ddf55668...\n",
            "[Round 9] val_acc=0.6472 val_f1=0.6285 time=55.03s\n",
            "\n",
            "--- ROUND 010 ---\n",
            "[Client 0] Training on 1567690 samples (round 10)\n",
            "[Blockchain] Block added for client 0 -> block_index=28 hash=eee9b9df75f0...\n",
            "[Client 1] Training on 1567690 samples (round 10)\n",
            "[Blockchain] Block added for client 1 -> block_index=29 hash=52c15d31ba1a...\n",
            "[Client 2] Training on 1567690 samples (round 10)\n",
            "[Blockchain] Block added for client 2 -> block_index=30 hash=96d4d440dd52...\n",
            "[Round 10] val_acc=0.6466 val_f1=0.6251 time=54.92s\n",
            "\n",
            "--- ROUND 011 ---\n",
            "[Client 0] Training on 1567690 samples (round 11)\n",
            "[Blockchain] Block added for client 0 -> block_index=31 hash=51e0442da3d5...\n",
            "[Client 1] Training on 1567690 samples (round 11)\n",
            "[Blockchain] Block added for client 1 -> block_index=32 hash=fefacace919e...\n",
            "[Client 2] Training on 1567690 samples (round 11)\n",
            "[Blockchain] Block added for client 2 -> block_index=33 hash=de1992454704...\n",
            "[Round 11] val_acc=0.6493 val_f1=0.6309 time=54.87s\n",
            "\n",
            "--- ROUND 012 ---\n",
            "[Client 0] Training on 1567690 samples (round 12)\n",
            "[Blockchain] Block added for client 0 -> block_index=34 hash=785cec27a595...\n",
            "[Client 1] Training on 1567690 samples (round 12)\n",
            "[Blockchain] Block added for client 1 -> block_index=35 hash=ebf61d4c11b2...\n",
            "[Client 2] Training on 1567690 samples (round 12)\n",
            "[Blockchain] Block added for client 2 -> block_index=36 hash=1dc13542de8c...\n",
            "[Round 12] val_acc=0.6497 val_f1=0.6315 time=54.98s\n",
            "\n",
            "--- ROUND 013 ---\n",
            "[Client 0] Training on 1567690 samples (round 13)\n",
            "[Blockchain] Block added for client 0 -> block_index=37 hash=fc56aeb78fd8...\n",
            "[Client 1] Training on 1567690 samples (round 13)\n",
            "[Blockchain] Block added for client 1 -> block_index=38 hash=ff68e75d85ac...\n",
            "[Client 2] Training on 1567690 samples (round 13)\n",
            "[Blockchain] Block added for client 2 -> block_index=39 hash=37884db3b819...\n",
            "[Round 13] val_acc=0.6504 val_f1=0.6331 time=54.93s\n",
            "\n",
            "--- ROUND 014 ---\n",
            "[Client 0] Training on 1567690 samples (round 14)\n",
            "[Blockchain] Block added for client 0 -> block_index=40 hash=7ad79633b299...\n",
            "[Client 1] Training on 1567690 samples (round 14)\n",
            "[Blockchain] Block added for client 1 -> block_index=41 hash=573cbc5543bf...\n",
            "[Client 2] Training on 1567690 samples (round 14)\n",
            "[Blockchain] Block added for client 2 -> block_index=42 hash=b6d3a9c658f8...\n",
            "[Round 14] val_acc=0.6524 val_f1=0.6338 time=54.52s\n",
            "\n",
            "--- ROUND 015 ---\n",
            "[Client 0] Training on 1567690 samples (round 15)\n",
            "[Blockchain] Block added for client 0 -> block_index=43 hash=96d6a53f6126...\n",
            "[Client 1] Training on 1567690 samples (round 15)\n",
            "[Blockchain] Block added for client 1 -> block_index=44 hash=30e4b9831fe8...\n",
            "[Client 2] Training on 1567690 samples (round 15)\n",
            "[Blockchain] Block added for client 2 -> block_index=45 hash=7f649fe0b068...\n",
            "[Round 15] val_acc=0.6515 val_f1=0.6339 time=54.69s\n",
            "\n",
            "--- ROUND 016 ---\n",
            "[Client 0] Training on 1567690 samples (round 16)\n",
            "[Blockchain] Block added for client 0 -> block_index=46 hash=fb0e71d521cb...\n",
            "[Client 1] Training on 1567690 samples (round 16)\n",
            "[Blockchain] Block added for client 1 -> block_index=47 hash=e6b5a4588796...\n",
            "[Client 2] Training on 1567690 samples (round 16)\n",
            "[Blockchain] Block added for client 2 -> block_index=48 hash=5a87685ecc5a...\n",
            "[Round 16] val_acc=0.6528 val_f1=0.6328 time=54.83s\n",
            "\n",
            "--- ROUND 017 ---\n",
            "[Client 0] Training on 1567690 samples (round 17)\n",
            "[Blockchain] Block added for client 0 -> block_index=49 hash=a3c026bc2ad6...\n",
            "[Client 1] Training on 1567690 samples (round 17)\n",
            "[Blockchain] Block added for client 1 -> block_index=50 hash=63276a23804f...\n",
            "[Client 2] Training on 1567690 samples (round 17)\n",
            "[Blockchain] Block added for client 2 -> block_index=51 hash=90c6f187c5b6...\n",
            "[Round 17] val_acc=0.6507 val_f1=0.6293 time=55.12s\n",
            "\n",
            "--- ROUND 018 ---\n",
            "[Client 0] Training on 1567690 samples (round 18)\n",
            "[Blockchain] Block added for client 0 -> block_index=52 hash=fe758d83e2e5...\n",
            "[Client 1] Training on 1567690 samples (round 18)\n",
            "[Blockchain] Block added for client 1 -> block_index=53 hash=0150aaa0289c...\n",
            "[Client 2] Training on 1567690 samples (round 18)\n",
            "[Blockchain] Block added for client 2 -> block_index=54 hash=937a117f4fe1...\n",
            "[Round 18] val_acc=0.6550 val_f1=0.6357 time=54.99s\n",
            "\n",
            "--- ROUND 019 ---\n",
            "[Client 0] Training on 1567690 samples (round 19)\n",
            "[Blockchain] Block added for client 0 -> block_index=55 hash=ca66dcd9b0cb...\n",
            "[Client 1] Training on 1567690 samples (round 19)\n",
            "[Blockchain] Block added for client 1 -> block_index=56 hash=e06361af10e1...\n",
            "[Client 2] Training on 1567690 samples (round 19)\n",
            "[Blockchain] Block added for client 2 -> block_index=57 hash=3ec8060dea45...\n",
            "[Round 19] val_acc=0.6528 val_f1=0.6317 time=54.72s\n",
            "\n",
            "--- ROUND 020 ---\n",
            "[Client 0] Training on 1567690 samples (round 20)\n",
            "[Blockchain] Block added for client 0 -> block_index=58 hash=35a7619e657b...\n",
            "[Client 1] Training on 1567690 samples (round 20)\n",
            "[Blockchain] Block added for client 1 -> block_index=59 hash=78a0f64d4938...\n",
            "[Client 2] Training on 1567690 samples (round 20)\n",
            "[Blockchain] Block added for client 2 -> block_index=60 hash=a52565220faa...\n",
            "[Round 20] val_acc=0.6563 val_f1=0.6357 time=54.79s\n",
            "\n",
            "--- ROUND 021 ---\n",
            "[Client 0] Training on 1567690 samples (round 21)\n",
            "[Blockchain] Block added for client 0 -> block_index=61 hash=a694fcfb4bb4...\n",
            "[Client 1] Training on 1567690 samples (round 21)\n",
            "[Blockchain] Block added for client 1 -> block_index=62 hash=9024b0be8334...\n",
            "[Client 2] Training on 1567690 samples (round 21)\n",
            "[Blockchain] Block added for client 2 -> block_index=63 hash=9a52843420d2...\n",
            "[Round 21] val_acc=0.6563 val_f1=0.6353 time=55.27s\n",
            "\n",
            "--- ROUND 022 ---\n",
            "[Client 0] Training on 1567690 samples (round 22)\n",
            "[Blockchain] Block added for client 0 -> block_index=64 hash=8c4276edc511...\n",
            "[Client 1] Training on 1567690 samples (round 22)\n",
            "[Blockchain] Block added for client 1 -> block_index=65 hash=dfa0a77b9617...\n",
            "[Client 2] Training on 1567690 samples (round 22)\n",
            "[Blockchain] Block added for client 2 -> block_index=66 hash=965a20f66c11...\n",
            "[Round 22] val_acc=0.6545 val_f1=0.6287 time=55.45s\n",
            "\n",
            "--- ROUND 023 ---\n",
            "[Client 0] Training on 1567690 samples (round 23)\n",
            "[Blockchain] Block added for client 0 -> block_index=67 hash=52d0ee7de781...\n",
            "[Client 1] Training on 1567690 samples (round 23)\n",
            "[Blockchain] Block added for client 1 -> block_index=68 hash=c60df9159168...\n",
            "[Client 2] Training on 1567690 samples (round 23)\n",
            "[Blockchain] Block added for client 2 -> block_index=69 hash=487f28cfe3c0...\n",
            "[Round 23] val_acc=0.6577 val_f1=0.6373 time=55.41s\n",
            "\n",
            "--- ROUND 024 ---\n",
            "[Client 0] Training on 1567690 samples (round 24)\n",
            "[Blockchain] Block added for client 0 -> block_index=70 hash=9d9c63ccfd9d...\n",
            "[Client 1] Training on 1567690 samples (round 24)\n",
            "[Blockchain] Block added for client 1 -> block_index=71 hash=51d3659bd61d...\n",
            "[Client 2] Training on 1567690 samples (round 24)\n",
            "[Blockchain] Block added for client 2 -> block_index=72 hash=f344a130f4d8...\n",
            "[Round 24] val_acc=0.6569 val_f1=0.6385 time=54.78s\n",
            "\n",
            "--- ROUND 025 ---\n",
            "[Client 0] Training on 1567690 samples (round 25)\n",
            "[Blockchain] Block added for client 0 -> block_index=73 hash=eb128a7e60c4...\n",
            "[Client 1] Training on 1567690 samples (round 25)\n",
            "[Blockchain] Block added for client 1 -> block_index=74 hash=c8c62b4ff246...\n",
            "[Client 2] Training on 1567690 samples (round 25)\n",
            "[Blockchain] Block added for client 2 -> block_index=75 hash=6f20a28301f3...\n",
            "[Round 25] val_acc=0.6592 val_f1=0.6366 time=54.50s\n",
            "\n",
            "--- ROUND 026 ---\n",
            "[Client 0] Training on 1567690 samples (round 26)\n",
            "[Blockchain] Block added for client 0 -> block_index=76 hash=f7d7e072b113...\n",
            "[Client 1] Training on 1567690 samples (round 26)\n",
            "[Blockchain] Block added for client 1 -> block_index=77 hash=7abdec062950...\n",
            "[Client 2] Training on 1567690 samples (round 26)\n",
            "[Blockchain] Block added for client 2 -> block_index=78 hash=5b4d4d4669c0...\n",
            "[Round 26] val_acc=0.6577 val_f1=0.6371 time=55.01s\n",
            "\n",
            "--- ROUND 027 ---\n",
            "[Client 0] Training on 1567690 samples (round 27)\n",
            "[Blockchain] Block added for client 0 -> block_index=79 hash=ec1e437cf1e3...\n",
            "[Client 1] Training on 1567690 samples (round 27)\n",
            "[Blockchain] Block added for client 1 -> block_index=80 hash=2fbb8c19b86a...\n",
            "[Client 2] Training on 1567690 samples (round 27)\n",
            "[Blockchain] Block added for client 2 -> block_index=81 hash=da0e11a5f1db...\n",
            "[Round 27] val_acc=0.6606 val_f1=0.6387 time=55.09s\n",
            "\n",
            "--- ROUND 028 ---\n",
            "[Client 0] Training on 1567690 samples (round 28)\n",
            "[Blockchain] Block added for client 0 -> block_index=82 hash=20c7930752d0...\n",
            "[Client 1] Training on 1567690 samples (round 28)\n",
            "[Blockchain] Block added for client 1 -> block_index=83 hash=7d8338b79ddc...\n",
            "[Client 2] Training on 1567690 samples (round 28)\n",
            "[Blockchain] Block added for client 2 -> block_index=84 hash=8fed6a83caef...\n",
            "[Round 28] val_acc=0.6582 val_f1=0.6381 time=54.72s\n",
            "\n",
            "--- ROUND 029 ---\n",
            "[Client 0] Training on 1567690 samples (round 29)\n",
            "[Blockchain] Block added for client 0 -> block_index=85 hash=d0142749d2a8...\n",
            "[Client 1] Training on 1567690 samples (round 29)\n",
            "[Blockchain] Block added for client 1 -> block_index=86 hash=1e98bf5eabf8...\n",
            "[Client 2] Training on 1567690 samples (round 29)\n",
            "[Blockchain] Block added for client 2 -> block_index=87 hash=767575bc695f...\n",
            "[Round 29] val_acc=0.6605 val_f1=0.6367 time=54.81s\n",
            "\n",
            "--- ROUND 030 ---\n",
            "[Client 0] Training on 1567690 samples (round 30)\n",
            "[Blockchain] Block added for client 0 -> block_index=88 hash=32955e40ec12...\n",
            "[Client 1] Training on 1567690 samples (round 30)\n",
            "[Blockchain] Block added for client 1 -> block_index=89 hash=826bb87abe2f...\n",
            "[Client 2] Training on 1567690 samples (round 30)\n",
            "[Blockchain] Block added for client 2 -> block_index=90 hash=9603f89c217b...\n",
            "[Round 30] val_acc=0.6599 val_f1=0.6369 time=54.68s\n",
            "\n",
            "--- ROUND 031 ---\n",
            "[Client 0] Training on 1567690 samples (round 31)\n",
            "[Blockchain] Block added for client 0 -> block_index=91 hash=10116742718c...\n",
            "[Client 1] Training on 1567690 samples (round 31)\n",
            "[Blockchain] Block added for client 1 -> block_index=92 hash=2384e7f5fd57...\n",
            "[Client 2] Training on 1567690 samples (round 31)\n",
            "[Blockchain] Block added for client 2 -> block_index=93 hash=b835edc9767d...\n",
            "[Round 31] val_acc=0.6612 val_f1=0.6359 time=54.68s\n",
            "\n",
            "--- ROUND 032 ---\n",
            "[Client 0] Training on 1567690 samples (round 32)\n",
            "[Blockchain] Block added for client 0 -> block_index=94 hash=30566ad11f02...\n",
            "[Client 1] Training on 1567690 samples (round 32)\n",
            "[Blockchain] Block added for client 1 -> block_index=95 hash=5b787735685b...\n",
            "[Client 2] Training on 1567690 samples (round 32)\n",
            "[Blockchain] Block added for client 2 -> block_index=96 hash=0964b43f9da5...\n",
            "[Round 32] val_acc=0.6637 val_f1=0.6370 time=54.67s\n",
            "\n",
            "--- ROUND 033 ---\n",
            "[Client 0] Training on 1567690 samples (round 33)\n",
            "[Blockchain] Block added for client 0 -> block_index=97 hash=7e4991161906...\n",
            "[Client 1] Training on 1567690 samples (round 33)\n",
            "[Blockchain] Block added for client 1 -> block_index=98 hash=e8923c545ff8...\n",
            "[Client 2] Training on 1567690 samples (round 33)\n",
            "[Blockchain] Block added for client 2 -> block_index=99 hash=e48b9fc49f22...\n",
            "[Round 33] val_acc=0.6632 val_f1=0.6377 time=54.64s\n",
            "\n",
            "--- ROUND 034 ---\n",
            "[Client 0] Training on 1567690 samples (round 34)\n",
            "[Blockchain] Block added for client 0 -> block_index=100 hash=384d2aeaa63d...\n",
            "[Client 1] Training on 1567690 samples (round 34)\n",
            "[Blockchain] Block added for client 1 -> block_index=101 hash=9efe7e41e832...\n",
            "[Client 2] Training on 1567690 samples (round 34)\n",
            "[Blockchain] Block added for client 2 -> block_index=102 hash=55833dc03293...\n",
            "[Round 34] val_acc=0.6631 val_f1=0.6406 time=54.73s\n",
            "\n",
            "--- ROUND 035 ---\n",
            "[Client 0] Training on 1567690 samples (round 35)\n",
            "[Blockchain] Block added for client 0 -> block_index=103 hash=df88cbb46576...\n",
            "[Client 1] Training on 1567690 samples (round 35)\n",
            "[Blockchain] Block added for client 1 -> block_index=104 hash=bcd829efe3f5...\n",
            "[Client 2] Training on 1567690 samples (round 35)\n",
            "[Blockchain] Block added for client 2 -> block_index=105 hash=67d1507c59ad...\n",
            "[Round 35] val_acc=0.6646 val_f1=0.6386 time=54.69s\n",
            "\n",
            "--- ROUND 036 ---\n",
            "[Client 0] Training on 1567690 samples (round 36)\n",
            "[Blockchain] Block added for client 0 -> block_index=106 hash=9f4e377299dc...\n",
            "[Client 1] Training on 1567690 samples (round 36)\n",
            "[Blockchain] Block added for client 1 -> block_index=107 hash=171f0383a5e4...\n",
            "[Client 2] Training on 1567690 samples (round 36)\n",
            "[Blockchain] Block added for client 2 -> block_index=108 hash=a8ff0720081d...\n",
            "[Round 36] val_acc=0.6622 val_f1=0.6328 time=54.44s\n",
            "\n",
            "--- ROUND 037 ---\n",
            "[Client 0] Training on 1567690 samples (round 37)\n",
            "[Blockchain] Block added for client 0 -> block_index=109 hash=ff285287ff1d...\n",
            "[Client 1] Training on 1567690 samples (round 37)\n",
            "[Blockchain] Block added for client 1 -> block_index=110 hash=da22ac292e01...\n",
            "[Client 2] Training on 1567690 samples (round 37)\n",
            "[Blockchain] Block added for client 2 -> block_index=111 hash=94f93b389a50...\n",
            "[Round 37] val_acc=0.6651 val_f1=0.6385 time=54.22s\n",
            "\n",
            "--- ROUND 038 ---\n",
            "[Client 0] Training on 1567690 samples (round 38)\n",
            "[Blockchain] Block added for client 0 -> block_index=112 hash=b56e3ddfc2e3...\n",
            "[Client 1] Training on 1567690 samples (round 38)\n",
            "[Blockchain] Block added for client 1 -> block_index=113 hash=f4f5e6655a6b...\n",
            "[Client 2] Training on 1567690 samples (round 38)\n",
            "[Blockchain] Block added for client 2 -> block_index=114 hash=ffa3c3b97f1e...\n",
            "[Round 38] val_acc=0.6649 val_f1=0.6388 time=54.69s\n",
            "\n",
            "--- ROUND 039 ---\n",
            "[Client 0] Training on 1567690 samples (round 39)\n",
            "[Blockchain] Block added for client 0 -> block_index=115 hash=c1b498360395...\n",
            "[Client 1] Training on 1567690 samples (round 39)\n",
            "[Blockchain] Block added for client 1 -> block_index=116 hash=a3375abe43db...\n",
            "[Client 2] Training on 1567690 samples (round 39)\n",
            "[Blockchain] Block added for client 2 -> block_index=117 hash=4b2373885d5d...\n",
            "[Round 39] val_acc=0.6662 val_f1=0.6382 time=54.78s\n",
            "\n",
            "--- ROUND 040 ---\n",
            "[Client 0] Training on 1567690 samples (round 40)\n",
            "[Blockchain] Block added for client 0 -> block_index=118 hash=32239be3da9f...\n",
            "[Client 1] Training on 1567690 samples (round 40)\n",
            "[Blockchain] Block added for client 1 -> block_index=119 hash=351bf0b7aa5c...\n",
            "[Client 2] Training on 1567690 samples (round 40)\n",
            "[Blockchain] Block added for client 2 -> block_index=120 hash=90f1b4eb7826...\n",
            "[Round 40] val_acc=0.6658 val_f1=0.6394 time=54.44s\n",
            "\n",
            "--- ROUND 041 ---\n",
            "[Client 0] Training on 1567690 samples (round 41)\n",
            "[Blockchain] Block added for client 0 -> block_index=121 hash=f832ea4b6871...\n",
            "[Client 1] Training on 1567690 samples (round 41)\n",
            "[Blockchain] Block added for client 1 -> block_index=122 hash=fe33d01c5cee...\n",
            "[Client 2] Training on 1567690 samples (round 41)\n",
            "[Blockchain] Block added for client 2 -> block_index=123 hash=aca556c2f33c...\n",
            "[Round 41] val_acc=0.6672 val_f1=0.6402 time=54.92s\n",
            "\n",
            "--- ROUND 042 ---\n",
            "[Client 0] Training on 1567690 samples (round 42)\n",
            "[Blockchain] Block added for client 0 -> block_index=124 hash=252f55c69487...\n",
            "[Client 1] Training on 1567690 samples (round 42)\n",
            "[Blockchain] Block added for client 1 -> block_index=125 hash=63b387392076...\n",
            "[Client 2] Training on 1567690 samples (round 42)\n",
            "[Blockchain] Block added for client 2 -> block_index=126 hash=26e5c8e052b0...\n",
            "[Round 42] val_acc=0.6650 val_f1=0.6381 time=54.24s\n",
            "\n",
            "--- ROUND 043 ---\n",
            "[Client 0] Training on 1567690 samples (round 43)\n",
            "[Blockchain] Block added for client 0 -> block_index=127 hash=7b40f9a136bd...\n",
            "[Client 1] Training on 1567690 samples (round 43)\n",
            "[Blockchain] Block added for client 1 -> block_index=128 hash=72ec566d715d...\n",
            "[Client 2] Training on 1567690 samples (round 43)\n",
            "[Blockchain] Block added for client 2 -> block_index=129 hash=3eaa40c424c9...\n",
            "[Round 43] val_acc=0.6694 val_f1=0.6429 time=54.22s\n",
            "\n",
            "--- ROUND 044 ---\n",
            "[Client 0] Training on 1567690 samples (round 44)\n",
            "[Blockchain] Block added for client 0 -> block_index=130 hash=f74aae011ab2...\n",
            "[Client 1] Training on 1567690 samples (round 44)\n",
            "[Blockchain] Block added for client 1 -> block_index=131 hash=821d6cfce0c8...\n",
            "[Client 2] Training on 1567690 samples (round 44)\n",
            "[Blockchain] Block added for client 2 -> block_index=132 hash=160450c93e73...\n",
            "[Round 44] val_acc=0.6674 val_f1=0.6398 time=54.60s\n",
            "\n",
            "--- ROUND 045 ---\n",
            "[Client 0] Training on 1567690 samples (round 45)\n",
            "[Blockchain] Block added for client 0 -> block_index=133 hash=5dded35136d3...\n",
            "[Client 1] Training on 1567690 samples (round 45)\n",
            "[Blockchain] Block added for client 1 -> block_index=134 hash=8b026c700635...\n",
            "[Client 2] Training on 1567690 samples (round 45)\n",
            "[Blockchain] Block added for client 2 -> block_index=135 hash=df0a9f1c0914...\n",
            "[Round 45] val_acc=0.6677 val_f1=0.6397 time=54.57s\n",
            "\n",
            "--- ROUND 046 ---\n",
            "[Client 0] Training on 1567690 samples (round 46)\n",
            "[Blockchain] Block added for client 0 -> block_index=136 hash=0d4bc960119c...\n",
            "[Client 1] Training on 1567690 samples (round 46)\n",
            "[Blockchain] Block added for client 1 -> block_index=137 hash=56eda7b66774...\n",
            "[Client 2] Training on 1567690 samples (round 46)\n",
            "[Blockchain] Block added for client 2 -> block_index=138 hash=ade63ab20e00...\n",
            "[Round 46] val_acc=0.6689 val_f1=0.6437 time=54.50s\n",
            "\n",
            "--- ROUND 047 ---\n",
            "[Client 0] Training on 1567690 samples (round 47)\n",
            "[Blockchain] Block added for client 0 -> block_index=139 hash=682a7312a4ac...\n",
            "[Client 1] Training on 1567690 samples (round 47)\n",
            "[Blockchain] Block added for client 1 -> block_index=140 hash=71b46af2870a...\n",
            "[Client 2] Training on 1567690 samples (round 47)\n",
            "[Blockchain] Block added for client 2 -> block_index=141 hash=12d2203a1fab...\n",
            "[Round 47] val_acc=0.6693 val_f1=0.6433 time=54.45s\n",
            "\n",
            "--- ROUND 048 ---\n",
            "[Client 0] Training on 1567690 samples (round 48)\n",
            "[Blockchain] Block added for client 0 -> block_index=142 hash=f6c5cb598480...\n",
            "[Client 1] Training on 1567690 samples (round 48)\n",
            "[Blockchain] Block added for client 1 -> block_index=143 hash=27dc4b4be110...\n",
            "[Client 2] Training on 1567690 samples (round 48)\n",
            "[Blockchain] Block added for client 2 -> block_index=144 hash=74f86f09dcb3...\n",
            "[Round 48] val_acc=0.6680 val_f1=0.6402 time=54.86s\n",
            "\n",
            "--- ROUND 049 ---\n",
            "[Client 0] Training on 1567690 samples (round 49)\n",
            "[Blockchain] Block added for client 0 -> block_index=145 hash=ce2245f47ed6...\n",
            "[Client 1] Training on 1567690 samples (round 49)\n",
            "[Blockchain] Block added for client 1 -> block_index=146 hash=dfdd613a41bb...\n",
            "[Client 2] Training on 1567690 samples (round 49)\n",
            "[Blockchain] Block added for client 2 -> block_index=147 hash=8939722a926d...\n",
            "[Round 49] val_acc=0.6688 val_f1=0.6428 time=54.96s\n",
            "\n",
            "--- ROUND 050 ---\n",
            "[Client 0] Training on 1567690 samples (round 50)\n",
            "[Blockchain] Block added for client 0 -> block_index=148 hash=4725e8c67b95...\n",
            "[Client 1] Training on 1567690 samples (round 50)\n",
            "[Blockchain] Block added for client 1 -> block_index=149 hash=7136b7220877...\n",
            "[Client 2] Training on 1567690 samples (round 50)\n",
            "[Blockchain] Block added for client 2 -> block_index=150 hash=f449379f5ad9...\n",
            "[Round 50] val_acc=0.6696 val_f1=0.6429 time=55.60s\n",
            "[INFO] Final global model saved to /content/drive/MyDrive/AUV Implenation/fl_blockchain_results/final_model.pth\n",
            "[INFO] Metrics saved to: /content/drive/MyDrive/AUV Implenation/fl_blockchain_results/federated_metrics_rounds.csv\n",
            "[INFO] Plots saved to /content/drive/MyDrive/AUV Implenation/fl_blockchain_results\n",
            "\n",
            "ALL DONE in 50.15 minutes. Final model: /content/drive/MyDrive/AUV Implenation/fl_blockchain_results/final_model.pth, metrics: /content/drive/MyDrive/AUV Implenation/fl_blockchain_results/federated_metrics_rounds.csv, ledger: /content/drive/MyDrive/AUV Implenation/fl_blockchain_results/ledger.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flwr pandas numpy scikit-learn matplotlib seaborn\n"
      ],
      "metadata": {
        "id": "hoiyxJfhznUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"flwr[simulation]\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QKxNgnXbz8WT",
        "outputId": "74eea592-8dab-45f3-f6f0-fe3806333151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flwr[simulation] in /usr/local/lib/python3.12/dist-packages (1.20.0)\n",
            "Requirement already satisfied: click<8.2.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (8.1.8)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (44.0.3)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (1.74.0)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.62.3 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (1.62.3)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.0.2)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (4.25.8)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (3.23.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (6.0.2)\n",
            "Requirement already satisfied: ray==2.31.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.31.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.32.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (0.12.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (3.19.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (25.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.4.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (1.17.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.19.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (4.14.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.27.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y flwr ray\n",
        "!pip install -U \"flwr[simulation]\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hzEkd4ya2aSy",
        "outputId": "1cb3ed2d-3ddd-4497-c4b1-9585c475080c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: flwr 1.20.0\n",
            "Uninstalling flwr-1.20.0:\n",
            "  Successfully uninstalled flwr-1.20.0\n",
            "Found existing installation: ray 2.31.0\n",
            "Uninstalling ray-2.31.0:\n",
            "  Successfully uninstalled ray-2.31.0\n",
            "Collecting flwr[simulation]\n",
            "  Using cached flwr-1.20.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: click<8.2.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (8.1.8)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (44.0.3)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (1.74.0)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.62.3 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (1.62.3)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.0.2)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (4.25.8)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (3.23.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (6.0.2)\n",
            "Collecting ray==2.31.0 (from flwr[simulation])\n",
            "  Using cached ray-2.31.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.32.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (0.12.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (3.19.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (25.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.4.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (1.17.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.19.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (4.14.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.27.0)\n",
            "Using cached ray-2.31.0-cp312-cp312-manylinux2014_x86_64.whl (66.7 MB)\n",
            "Using cached flwr-1.20.0-py3-none-any.whl (617 kB)\n",
            "Installing collected packages: ray, flwr\n",
            "Successfully installed flwr-1.20.0 ray-2.31.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flwr",
                  "ray"
                ]
              },
              "id": "9bfd02db30194a7997ee2fdca350c428"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "print(ray.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LecVtOO20wT0",
        "outputId": "69092b14-21a8-4c6b-bbe7-4ad5bf6994a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "print(\"Ray version:\", ray.__version__)\n",
        "import flwr\n",
        "print(\"Flower version:\", flwr.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqNBi0L22pkX",
        "outputId": "f292f8ec-b7b2-43d2-8afc-991891f9a7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ray version: 2.31.0\n",
            "Flower version: 1.20.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.cuda as cuda\n",
        "import flwr as fl\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from typing import Dict\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ----------------------------\n",
        "# GOOGLE COLAB / MULTI-CLIENT CONFIG\n",
        "# ----------------------------\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG\n",
        "# ----------------------------\n",
        "CSV_PATH = \"/content/drive/MyDrive/AUV Implenation/balanced_sample_200MB.csv\"\n",
        "NUM_CLIENTS_REQUESTED = 4      # simulate clients\n",
        "BATCH_SIZE = 512                 # reduce per-client batch size\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "NUM_ROUNDS = 20                  # reduce for testing on Colab\n",
        "LEARNING_RATE = 0.001\n",
        "PIN_MEMORY = True\n",
        "SAVE_DIR = \"/content/drive/MyDrive/AUV Implenation\"\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# ----------------------------\n",
        "# DEVICE SETUP\n",
        "# ----------------------------\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"CUDA not available!\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "props = torch.cuda.get_device_properties(0)\n",
        "print(f\"GPU Memory: {props.total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "torch.cuda.set_per_process_memory_fraction(0.85)\n",
        "\n",
        "# ----------------------------\n",
        "# METRICS TRACKER\n",
        "# ----------------------------\n",
        "class MetricsTracker:\n",
        "    def __init__(self):\n",
        "        self.client_metrics = defaultdict(list)\n",
        "        self.global_metrics = []\n",
        "        self.round_times = []\n",
        "\n",
        "    def add_client_metrics(self, client_id: int, round_num: int, metrics: Dict):\n",
        "        entry = {'client_id': client_id, 'round': round_num, **metrics}\n",
        "        self.client_metrics[client_id].append(entry)\n",
        "\n",
        "    def add_global_metrics(self, round_num: int, metrics: Dict):\n",
        "        entry = {'round': round_num, **metrics}\n",
        "        self.global_metrics.append(entry)\n",
        "\n",
        "    def save_all(self, save_dir: str):\n",
        "        if self.client_metrics:\n",
        "            pd.concat([pd.DataFrame(v) for v in self.client_metrics.values()]).to_csv(\n",
        "                f\"{save_dir}/client_metrics.csv\", index=False\n",
        "            )\n",
        "        if self.global_metrics:\n",
        "            pd.DataFrame(self.global_metrics).to_csv(f\"{save_dir}/global_metrics.csv\", index=False)\n",
        "        print(f\"[METRICS] Saved metrics to {save_dir}\")\n",
        "\n",
        "metrics_tracker = MetricsTracker()\n",
        "\n",
        "# ----------------------------\n",
        "# LOAD DATA (once)\n",
        "# ----------------------------\n",
        "print(\"[INFO] Loading dataset...\")\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "df = df.drop(columns=[\"Date\", \"Time (UTC)\"], errors=\"ignore\")\n",
        "df = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
        "\n",
        "X = df.iloc[:, :-1].values.astype(np.float32)\n",
        "y = df.iloc[:, -1].values.astype(np.int64)\n",
        "dataset = TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
        "total_len = len(dataset)\n",
        "print(f\"[INFO] Samples: {total_len}\")\n",
        "\n",
        "# free temporary arrays\n",
        "del X, y, df\n",
        "gc.collect()\n",
        "\n",
        "# ----------------------------\n",
        "# CLIENT SPLIT (once) + PREPARE LOADERS ONCE\n",
        "# ----------------------------\n",
        "all_indices = np.arange(total_len)\n",
        "np.random.seed(SEED)\n",
        "np.random.shuffle(all_indices)\n",
        "\n",
        "client_splits = np.array_split(all_indices, NUM_CLIENTS_REQUESTED)\n",
        "client_indices_list = [arr.tolist() for arr in client_splits if arr.size > 0]\n",
        "NUM_CLIENTS = len(client_indices_list)\n",
        "print(f\"[INFO] Using {NUM_CLIENTS} clients\")\n",
        "\n",
        "# Pre-create train/test loaders for each client (so we don't rebuild them every round)\n",
        "client_train_loaders = []\n",
        "client_test_loaders = []\n",
        "\n",
        "for idxs in client_indices_list:\n",
        "    n = len(idxs)\n",
        "    train_size = max(1, int(0.8 * n))\n",
        "    # deterministic split using the shuffled indices order\n",
        "    train_idxs = idxs[:train_size]\n",
        "    test_idxs = idxs[train_size:] if train_size < n else []\n",
        "\n",
        "    train_subset = Subset(dataset, train_idxs)\n",
        "    if test_idxs:\n",
        "        test_subset = Subset(dataset, test_idxs)\n",
        "    else:\n",
        "        # if no test split (very small partition), reuse a tiny slice from train to avoid empty test set\n",
        "        # (keeps behavior safe)\n",
        "        test_subset = Subset(dataset, train_idxs[:1])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_subset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    client_train_loaders.append(train_loader)\n",
        "    client_test_loaders.append(test_loader)\n",
        "\n",
        "# ----------------------------\n",
        "# MODEL\n",
        "# ----------------------------\n",
        "class OptimizedNet(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# ----------------------------\n",
        "# CLIENT CLASS\n",
        "# ----------------------------\n",
        "class EnhancedFlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, model, trainloader, testloader, cid):\n",
        "        self.model = model.to(DEVICE)\n",
        "        self.trainloader, self.testloader = trainloader, testloader\n",
        "        self.cid = cid\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    def get_parameters(self, config=None):\n",
        "        return [v.cpu().numpy() for v in self.model.state_dict().values()]\n",
        "\n",
        "    def set_parameters(self, params):\n",
        "        state_dict = {k: torch.tensor(v, device=DEVICE) for k, v in zip(self.model.state_dict().keys(), params)}\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def fit(self, params, config):\n",
        "        self.set_parameters(params)\n",
        "        self.model.train()\n",
        "        for data, target in self.trainloader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.criterion(self.model(data), target)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        return self.get_parameters(), len(self.trainloader.dataset), {}\n",
        "\n",
        "    def evaluate(self, params, config):\n",
        "        self.set_parameters(params)\n",
        "        self.model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in self.testloader:\n",
        "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "                pred = self.model(data).argmax(dim=1)\n",
        "                correct += (pred == target).sum().item()\n",
        "                total += target.size(0)\n",
        "        return float(0.0), total, {\"accuracy\": correct / total}\n",
        "\n",
        "# ----------------------------\n",
        "# CLIENT FACTORY (uses pre-created loaders)\n",
        "# ----------------------------\n",
        "def create_client(cid):\n",
        "    # Reuse the loaders created once above to avoid reloading/duplicating the dataset\n",
        "    train_loader = client_train_loaders[cid]\n",
        "    test_loader = client_test_loaders[cid]\n",
        "    input_dim = dataset[0][0].shape[0]\n",
        "    num_classes = len(torch.unique(dataset[:][1]))\n",
        "    return EnhancedFlowerClient(OptimizedNet(input_dim, num_classes), train_loader, test_loader, cid)\n",
        "\n",
        "def client_fn(context: fl.common.Context):\n",
        "    cid = int(context.node_id) % NUM_CLIENTS\n",
        "    return create_client(cid).to_client()\n",
        "\n",
        "# ----------------------------\n",
        "# TRAINING\n",
        "# ----------------------------\n",
        "def run_federated_training():\n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=1.0,\n",
        "        min_fit_clients=NUM_CLIENTS,\n",
        "        min_evaluate_clients=NUM_CLIENTS,\n",
        "        min_available_clients=NUM_CLIENTS,\n",
        "    )\n",
        "    fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=NUM_CLIENTS,\n",
        "        config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
        "        strategy=strategy,\n",
        "        client_resources={\"num_cpus\": 1, \"num_gpus\": 0.01},  # tiny GPU fraction per client\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_federated_training()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foQFa8Qfy2wV",
        "outputId": "7b63f2a4-194f-4669-b2ca-e74924132995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 42.47 GB\n",
            "[INFO] Loading dataset...\n",
            "[INFO] Samples: 5225634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
            "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
            "\n",
            "\t\t$ flwr new  # Create a new Flower app from a template\n",
            "\n",
            "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
            "\n",
            "\tUsing `start_simulation()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=20, no round_timeout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using 4 clients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-26 10:21:36,675\tINFO worker.py:1771 -- Started a local Ray instance.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:A100': 1.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'object_store_memory': 26799219916.0, 'node:172.28.0.12': 1.0, 'memory': 53598439835.0, 'GPU': 1.0}\n",
            "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.01}\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 12 actors\n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "\u001b[36m(pid=23012)\u001b[0m 2025-08-26 10:21:39.520910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=23012)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=23012)\u001b[0m E0000 00:00:1756203699.558069   23012 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=23012)\u001b[0m E0000 00:00:1756203699.569362   23012 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=23012)\u001b[0m W0000 00:00:1756203699.597629   23012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=23012)\u001b[0m W0000 00:00:1756203699.597668   23012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=23012)\u001b[0m W0000 00:00:1756203699.597674   23012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=23012)\u001b[0m W0000 00:00:1756203699.597679   23012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[36m(ClientAppActor pid=23016)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=23016) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=23016)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(pid=23016)\u001b[0m 2025-08-26 10:21:39.612279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=23016)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=23016)\u001b[0m E0000 00:00:1756203699.647993   23016 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=23016)\u001b[0m E0000 00:00:1756203699.657428   23016 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=23016)\u001b[0m W0000 00:00:1756203699.681061   23016 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No evaluate_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 6]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 7]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 8]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 9]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 10]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 11]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 12]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 13]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 14]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 15]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 16]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 17]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 18]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 19]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 20]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
            "\u001b[92mINFO \u001b[0m:      Run finished 20 round(s) in 516.84s\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 4: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 5: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 6: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 7: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 8: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 9: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 10: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 11: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 12: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 13: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 14: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 15: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 16: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 17: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 18: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 19: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 20: 0.0\n",
            "\u001b[92mINFO \u001b[0m:      \n"
          ]
        }
      ]
    }
  ]
}